{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU, SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import BatchNormalization\nfrom keras.utils import np_utils \nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","metadata":{"execution":{"iopub.status.busy":"2023-10-25T21:28:55.075728Z","iopub.execute_input":"2023-10-25T21:28:55.076171Z","iopub.status.idle":"2023-10-25T21:28:57.695584Z","shell.execute_reply.started":"2023-10-25T21:28:55.076137Z","shell.execute_reply":"2023-10-25T21:28:57.693456Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Running on TPU: \", tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T21:58:44.284710Z","iopub.execute_input":"2023-10-25T21:58:44.285152Z","iopub.status.idle":"2023-10-25T21:58:44.298972Z","shell.execute_reply.started":"2023-10-25T21:58:44.285117Z","shell.execute_reply":"2023-10-25T21:58:44.298050Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(\"\") # Jigsaw toxic comment\nvalidation = pd.read_csv(\"\")\ntest = pd.read_csv(\"\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Approaching this problem as a binary classification (12000 data points to speed up training)","metadata":{}},{"cell_type":"code","source":"train.drop(['servere_toxic', 'obscene','threat','insult','identity_hate'], axis = 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.loc[:12000, :]\ntrain.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['comment_text'].apply(lambda x: len(str(x).split())).max()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting AUC score\n\ndef roc_auc(predictions, target):\n    \"\"\"\n    This method returns the AUC Score when given the Predictions and Labels\n    \"\"\"\n    fpr,tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Preparation\n","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train.comment_text.values, \n                                                      train.toxic.values,\n                                                     stratify = train.toxic.values, \n                                                      random_state = 42, \n                                                      test_size = 0.2,\n                                                     shuffle = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple RNN\n\nRecurrent Neural Network (RNN): type of NN where the output from previous step are fed as input to the current step. In traditional NN all inputs and outputs are independent since language is related is important to know the previous words in the sentence in order to understand the context.","metadata":{}},{"cell_type":"code","source":"# Keras tokenizer\n\ntoken = text.Tokenizer(num_words = None)\nmax_len = 1500\n\ntoken.fit_on_texts(list(X_train) + list(X_valid))\nX_train_seq = token.texts_to_sequences(X_train)\nX_valid_seq = token.texts_to_sequences(X_valid)\n\n# Zero padding the sequences \nX_train_pad = sequence.pad_sequences(X_train_seq, maxlen = maxlen)\nX_valid_pad = sequence.pad_sequences(X_train_seq, maxlen = maxlen)\n\nword_index = token.word_index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nwith strategy.scope():\n    # A simple RNN w/o any pretrained embeddings and dense layers\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                       300,\n                       input_length = max_len))\n    model.add(SimpleRNN(100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T21:58:55.660085Z","iopub.execute_input":"2023-10-25T21:58:55.660453Z","iopub.status.idle":"2023-10-25T21:58:55.780968Z","shell.execute_reply.started":"2023-10-25T21:58:55.660423Z","shell.execute_reply":"2023-10-25T21:58:55.780136Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m<timed exec>:4\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"],"ename":"NameError","evalue":"name 'word_index' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model.fit(X_train_pad, y_train, nb_epoch=5, batch_size=64*strategy.num_replicas_in_sync)\n# Multiplying by strategy to run on TPU's","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(X_valid_pad)\nprint(f\"AUC: {roc_auc(scores, y_valid):.2f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_model = []\nscores_model.append({\"Model\": \"SimpleRNN\", \"AUC_Score\" : roc_auc(scores, y_valid)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Explanation \n\n- Tokenization \n\nA sentence is inputted word by word. Each word is represented as one hot encoded vector of dimensions (Number of words in vocab + 1).\nThe Keras tokenizer work by taking all of the unique words in the text, forms a dictionary with words as keys and their frequency in the texts as values. The dictionary is then sorted in descending order of counts. ","metadata":{}},{"cell_type":"code","source":"X_train_seq[:1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Padding** the sequence allows for batch training and can help prevent the model to overfitting on the data. It is efficient to process data in batches this is done by matrices `[batch_size * sequence_length]`. If there's a variation in the `sequence_length` then it'll be the longest sequence, so the sequences are padded with 0s to fit the matrix size and its masked to not be accounted for in loss calculation. \nPadded tokens may not impact model training it still requires compute to be processed and sorting data by length helps limit the amount of padding.\n\nAlso can use special tokens while tokenizing for beginning of string (BOS) and end of string (EOS). This is done so the knows when to stop especially if the response has multiple sentences and thus a period token would not be sufficient. \nThe BOS token allows for the model to choose the first word when being told to do a task where it simply is not completing a sentence, like writing \"original\" poetry.","metadata":{}},{"cell_type":"markdown","source":"`model.Sequential()` tells keras that we will be bulding the Network Sequentially. We start by adding the Embedding Layer of neurons which takes in as input the nth dimensional One-hot vector of every word and converts it into 300 dimensional vector, it gives us word embeddings similar to `word2vec`. Could've used `word2vec` for this but the `Embedding` layer learns during the training to enhance the embeddings. Next we added the 100 LSTM units without any dropout or regularization. In the end a singlue neuron sigmoid function that takes outputs from 100 LSTM cells (These are 100 LSTM cells not layers) for predicting the results and then compiling the model using `Adam` optimizer. \n\n","metadata":{}},{"cell_type":"markdown","source":"### Word Embeddings\n\nWord embeddings is a learned representation for text where words that have the same meaning have a similar representation. \n\nIt's easier to use pretrained models like GLoVe, Word2Vec, fasttext. ","metadata":{}},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open('kaggle/input/glove840b300dtxt/GLoVe.840B.300d.txt',\n        'r', encoding = 'utf-8') # kaggle/input/glove840b300dtxt/GLoVe.840B.300d.txt\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint(f\"Found {embeddings_index} word vectors.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM\n\n#### Overview \nSimple RNNs perform better than classical ML algos but they fail to capture the long term dependencies that is present in sentences. 1998-99 LSTMs were introduced to counteract these inefficiencies. \n\nRNNs also have an issue with vanishing gradients","metadata":{}},{"cell_type":"code","source":"# create an embedding matric for the words we have in the dataset\nembedding_matrix = np.zeros(len(word_index) + 1, 300)\nfor word, i in tqdm(word_index.items()):\n    embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nwith strategy.scope():\n    \n    # A simple LSTM with GLoVe embeddings and one dense layer\n    \n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1, 300,\n             weights = [embedding_matrix],\n             input_length = max_len,\n             trainable = False))\n    model.add(LSTM(100, dropout = 0.3, recurrent_dropout = 0.3))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.comile(loss = 'binary-crossentropy', \n                optimizer = 'adam',\n                metrics = ['accuracy'])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train_pad, y_train, \n          epochs=5, \n          batch_size = 64*strategy.num_replicas_in_sync)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(X_valid_pad)\nprint(f\"AUC: {roc_auc(scores, y_valid)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_model.append({\"Model\" : \"LSTM\",\n                    \"AUC_Score\" : roc_auc(scores, y_valid)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explanation\n\nCalculated the embeddings matrix for the vocabulary from the pretrained GLoVe vectors. In the Sequential model while building the Embedding layers the Embedding matrix is passed as weights to the layer instead of training it over the Vocabulary and so `trainable = False`. Same as RNN but replaced with LSTM units.","metadata":{}},{"cell_type":"markdown","source":"### GRUs\n\nGated Recurrent Unit is designed to solve the vanishing gradient problem which comes with a standard RNN. GRU is a variation on the LSTM b/c both share a similar design but is suppose to be simpler and faster than LSTMs and in many cases produce equally good results.","metadata":{}},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    # GRU with GLoVe embdeddings and 2 dense layers \n    model = model.Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                       300,\n                       weights=[embedding_matrix],\n                       input_length=max_len,\n                       trainable = False))\n    model.add(SpatialDropout1D(0.3))\n    model.add(GRU(300))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                 optimizer='adam',\n                 metrics=['accuracy'])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train_pad, y_train, epochs=5,\n         batch_size = 64*strategy.num_replicas_in_sync)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(X_valid_pad)\nprint(f\"AUC: {roc_auc(scores, y_valid)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_model.append({\"Model\": \"GRU\",\n                    \"AUC Score\": roc_auc(scores, y_valid)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bi-Directional RNNs\n","metadata":{}},{"cell_type":"code","source":"%%time \nwith strategy.scope():\n    # Bidirectional LSTM with GLoVe embeddings & 1 Dense Layer\n    model = Sequential()\n    model.add(Embeddings(len(word_index) + 1,\n                        300,\n                        weights=[embedding_matrix],\n                        input_length=max_len,\n                        trainable = False))\n    model.add(Bidirectional(LSTM(300, dropout = 0.3,\n                                recurrent_dropout = 0.3)))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(loss='binary_crossentropy',\n                 optimizer='adam',\n                 metrics=['accuracy'])\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train_pad, y_train, epochs=5, batch_size= 64*strategy.num_replicas_in_sync)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.predict(X_valid_pad)\nprint(f\"AUC: {roc_auc(scores,y_valid)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_model.append({\"Model\" : \"Bi-directional LSTM\",\n                    \"AUC_Score\" : roc_auc(scores, y_valid)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Explanation \n\nAdded Bidirection to the LSTM layer.","metadata":{}},{"cell_type":"markdown","source":"### Seq2Seq Model Architecture\n\n#### Overview \n\nA many to many RNN architecture where the input is a sequence and the output is also a sequence (Where the input and output sequence can be of different lengths). Generally used in applications like Machine Translation, text summarization, question answering etc.","metadata":{}},{"cell_type":"code","source":"# Visualizing results obtained from various Deep learning models \nresults = pd.DataFrame(scores_model).sort_values(by='AUC_Score', ascending = False)\nresult.style.background_gradient(cmap='Blues')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(go.Funnelarea(\ntext = results.Model,\nvalues = results.AUC_Score,\ntitle = {\"position\":\"top center\", \"text\" : \"Funnel-Chart of Sentiment Distribution\"}))\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attention Models","metadata":{}},{"cell_type":"code","source":"import os \nimport tensorflow as tf \nfrom tensorflow.keras.layer import Dense, Input\nfrom tensorflow.keras.optimizer import Adam\nfrom tensorflow.keras.models import Model\nfrom tesnorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_dataset import KaggleDatasets\nimport transformers\n\nfrom tekenizer import BertWordPieceTokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train\nvalid\ntest","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size = 256, maxlen = 512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length = maxlen)\n    tokenizer.enable_padding(max_length = maxlen)\n    all_ids=[]\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n        \n    return np.array(all_ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMP Data for Config\n\nAUTO = tf.data.experimental.AUTOTUNE\n\n# configuration\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenization","metadata":{}}]}