{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n\nfrom IPython.display import display_html, clear_output, Markdown\nfrom gc import collect\nimport copy, pandas as pd, numpy as np, joblib, ctypes\nfrom os import system, getpid, walk\nfrom psutil import Process\nfrom copy import deepcopy\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nfrom tqdm.notebook import tqdm\nfrom pprint import pprint \nfrom colorama import Fore, Style, init\n\nimport lightgbm\nfrom itertools import combinations \nfrom sklearn.model_selection import KFold\n\ncollect()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:20:13.190415Z","iopub.execute_input":"2023-10-30T14:20:13.190772Z","iopub.status.idle":"2023-10-30T14:20:14.919995Z","shell.execute_reply.started":"2023-10-30T14:20:13.190745Z","shell.execute_reply":"2023-10-30T14:20:14.918924Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CPU times: user 1.15 s, sys: 150 ms, total: 1.3 s\nWall time: 1.72 s\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}]},{"cell_type":"code","source":"%%time \n\nfrom sklearn.model_selection import (RepeatedStratifiedKFold as RSKF,\n                                    StratifiedKFold as SKF,\n                                    KFold,\n                                    RepeatedKFold as RKF,\n                                    cross_val_score)\nfrom lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR\nfrom xgboost import XGBRegressor as XGBR\nfrom catboost import CatBoostRegressor as CBR\nfrom sklearn.ensemble import HistGradientBoostingRegressor as HGBR\nfrom sklearn.metrics import mean_absolute_error as mae, make_scorer\n\ncollect()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T14:36:18.834996Z","iopub.execute_input":"2023-10-30T14:36:18.835370Z","iopub.status.idle":"2023-10-30T14:36:19.780709Z","shell.execute_reply.started":"2023-10-30T14:36:18.835341Z","shell.execute_reply":"2023-10-30T14:36:19.779250Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CPU times: user 404 ms, sys: 64.5 ms, total: 468 ms\nWall time: 937 ms\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"164"},"metadata":{}}]},{"cell_type":"code","source":"%%time \n\ndef GetMemUsage():\n    \"\"\"\n    defines the memory usage across the kernel.\n    \"\"\"\n    \n    pid = getpid()\n    py = Process(pid)\n    memory_use = py.memory_info()[0] / 2. * 30\n    return f\"RAM memory GB usage = {memory_use :.4f}\"\n\nfrom sklearn import set_config\nset_config(transform_output = \"pandas\")","metadata":{"execution":{"iopub.status.busy":"2023-10-30T15:41:39.307077Z","iopub.execute_input":"2023-10-30T15:41:39.307414Z","iopub.status.idle":"2023-10-30T15:41:39.313638Z","shell.execute_reply.started":"2023-10-30T15:41:39.307391Z","shell.execute_reply":"2023-10-30T15:41:39.312670Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CPU times: user 77 µs, sys: 19 µs, total: 96 µs\nWall time: 102 µs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Configuration Class","metadata":{}},{"cell_type":"code","source":"%%time \n\nclass CFG:\n    \"\"\"\n    Configuration class for parameters and CV strategy for tuning and trainig\n    use CAPITAL LETTERS when filling in parameters.\n    \"\"\"\n    \n    # Data preparation:\n    version_nb = 5\n    test_req = \"N\"\n    test_frac = 0.01\n    load_tr_data = \"N\"\n    gpu_switch = \"OFF\"\n    state = 42\n    target = 'target'\n    \n    path = f'/kaggle/input/optiver-memoryreducedatasets/'\n    test_path = f'/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv'\n    df_choice = f'XTrIntCmpNewFtre.parquet'\n    mdl_path = f'/kaggle/working/BaselineML/'\n    inf_path = f'/kaggle/input/optiverbaselinemodels/'\n    \n    # Model Training:\n    methods = ['LGBMR', 'CBR', 'HGBR']\n    ML = \"N\"\n    n_splits = 5\n    n_repeats = 1\n    nbrnd_erly_stp = 100\n    mdlcv_mthd = 'KF'\n    \n    # Ensemble\n    ensemble_req = \"N\"\n    enscv_mthd = \"KF\"\n    metric_obj = 'minimize'\n    ntrials = 10 if test_req == \"Y\" else 200\n    ens_weights = [0.54, 0.44, 0.02]\n    \n    # Inference:\n    inference_req = \"Y\"\n    \n    # Global variables for plotting: \n    grid_specs = {'visible' : True, 'which' : 'both', 'linestyle' : '--',\n                 'color' : 'lightgrey', 'linewidth' : 0.75}\n    title_specs = {\"fontsize\" : 9, 'fontweight' : 'bold', 'color' : 'tab:blue'}\n    \n    print(\"Configuration done!\")\n    collect()\n    print(GetMemUsage())","metadata":{"execution":{"iopub.status.busy":"2023-10-30T15:41:44.208719Z","iopub.execute_input":"2023-10-30T15:41:44.209181Z","iopub.status.idle":"2023-10-30T15:41:44.522075Z","shell.execute_reply.started":"2023-10-30T15:41:44.209152Z","shell.execute_reply":"2023-10-30T15:41:44.520690Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Configuration done!\nRAM memory GB usage = 4458577920.0000\nCPU times: user 300 ms, sys: 478 µs, total: 300 ms\nWall time: 300 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Cross validation \n","metadata":{}},{"cell_type":"code","source":"%%time \n\nall_cv = {\"KF\" : KFold(n_splits = CFG.n_splits, shuffle = True, random_state = CFG.state),\n         'RKF' : RKF(n_splits = CFG.n_splits, n_repeats = CFG.n_repeats ,random_state = CFG.state),\n         'RSKF' : RSKF(n_splits = CFG.n_splits, n_repeats = CFG.n_repeats, random_state = CFG.state),\n         'SKF' : SKF(n_splits = CFG.n_splits, shuffle = True, random_state = CFG.state)}\n\n# Defining the metric\ndef ScoreMetric(y_true, y_pred) -> float:\n    \"\"\"\n    Calculates the metric for the competition \n    y_true = ground truth np.array\n    y_pred = predictions\n    return = metric value (float)\n    \"\"\"\n    return mae(y_true, y_pred)\n\n# Custom scorer for cross_val_predict\nmyscorer = make_scorer(ScoreMetric, greater_is_better = False, needs_proba = False)\n\ncollect()\nprint(GetMemUsage())","metadata":{"execution":{"iopub.status.busy":"2023-10-30T15:42:43.204040Z","iopub.execute_input":"2023-10-30T15:42:43.204376Z","iopub.status.idle":"2023-10-30T15:42:43.374677Z","shell.execute_reply.started":"2023-10-30T15:42:43.204352Z","shell.execute_reply":"2023-10-30T15:42:43.373102Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"RAM memory GB usage = 4458577920.0000\nCPU times: user 164 ms, sys: 401 µs, total: 164 ms\nWall time: 162 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Conversion and Adjustment","metadata":{}},{"cell_type":"code","source":"%%time \n\ndef goto_conversion(listOfOdds, total = 1, eps = 1e-6, isAmericanOdds = False):\n    \n    \"\"\"\n    Function for converting odds to probabilities\n    \n    The function first checks if the odds are in American format \n    and if so converts them to decimal odds if needed. Then computes the \n    probabilities based on the inverse of the odds. After this calculates the\n    standard error (SE) for each probability. Ends with adjusting probabilities\n    by stepping back based on the SE to ensure the sum is equal to the `total`.\n    \n    returns adjusted probabilities.\n    =====================================================\n    Parameters\n    _____________________________________________________\n    listOfOdds: list\n        a list of odds that is either in American or decimal format\n    total: float\n        total sum of probabilities\n    eps: float\n        small value to prevent division by 0\n    isAmericanOdds: bool\n        A boolean flag indicating whether the input odds are in American format\n    \n    \"\"\"\n    \n    # Converting American odds to Decimal odds \n    if isAmericanOdds:\n        for i in range(len(listOfOdds)):\n            currOdds = listOfOdds[i]\n            isNegativeAmericanOdds = currOdds < 0\n            if isNegativeAmericanOdds:\n                currDecimalOdds = 1 + (100/(currOdds*-1))\n            else:\n                # Is non-negative 'merican odds\n                currDecimalOdds = 1 + (currOdds/100)\n            listOfOdds[i] = currDecimalOdds\n        \n    # Error catchers \n    if len(listOfOdds) < 2:\n        raise ValueError('len(listOfOdds) must be >= 2')\n    if any(x < 1 for x in listOfOdds):\n        raise ValueError(\"All odds must be >= 1, set isAmericanOdds parameter to True if using American Odds.\")\n    \n    # Computation:\n    # init probabilities using inverse odds\n    listOfProbabilities = [1/x for x in listOfOdds]\n    \n    # compute the standard error (SE) for each proabilitiy\n    listOfSe = [pow((x-x**2)/x, 0.5) for x in listOfProbabilities]\n    \n    # compute how many steps for SE the probabilityies should step back by\n    step = (sum(listOfProbabilities) - total)/sum(listOfSe)\n    \n    outputListOfProbabilities = [min(max(x - (y*step), eps), 1) for x,y in zip(listOfProbabilities, listOfSe)]\n    \n    return outputListOfProbabilities\n\ndef zero_sum(listOfProces, listOfVolumes):\n    \"\"\"\n    Adjusts the list of prices and volumes to achieve a zero-sum condition.\n    -----------------------------------------------------------------------\n    Parameters\n    ----------\n    listOfPrices: list of the prices\n    listOfVolumes: list of volumes corresponding to the prices.\n    \n    =======================================================================\n    Computes Standard Error (SE) for each price based on the volumes.\n    Then adjusts the prices by scaling them using the standard errors to achieve\n    a zero-sum condition.\n    \n    returns adjusted prices\n    \n    \"\"\"\n    # compute standard errors assuming standard deviation is same for all stocks \n    listOfSe = [x**0.5 for x in listOfVolumes]\n    step = sum(listOfPrices)/sum(listofSe)\n    outputListOfPrices = [x - (y*step) for x,y in zip(listOfPrices,listOfSe)]\n    return outputListOfPrices\n\ncollect()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T16:35:12.296836Z","iopub.execute_input":"2023-10-30T16:35:12.298115Z","iopub.status.idle":"2023-10-30T16:35:12.587713Z","shell.execute_reply.started":"2023-10-30T16:35:12.298021Z","shell.execute_reply":"2023-10-30T16:35:12.587007Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CPU times: user 274 ms, sys: 0 ns, total: 274 ms\nWall time: 273 ms\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load and Prepare training \n","metadata":{}},{"cell_type":"code","source":"%%time \n\nif (CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\") and CFG.test_req == 'Y':\n    if isinstance(CFG.test_frac, float):\n        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(frac = CFG.test_frac)\n    else:\n        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(n = CFG.test_frac)\n    \n    y = pd.read_parquet(CFG.path + f\"Ytrain.parquet\").loc[X.index].squeeze()\n    print(f\"Sampled train shapes for coding tests = {X.shape} {y.shape}\")\n    \n    X.index, y.index = range(len(X)), range(len(y))\n    \n    print(X.columns)\n    \nelif CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\":\n    X = pd.read_parquet(CFG.path + CFG.df_choice)\n    y = pd.read_parquet(CFG.path + \"Ytrain.parquet\").squeeze()\n    \n    print(f\"Train shapes for code testing = {X.shape} {y.shape}\")\n    \nelif CFG.load_tr_data != \"Y\" or CFG.inference_req == \"Y\":\n    print(f\"Train data is not required since inferring from the model\")\n    \ncollect()\nlibc.malloc_trim(0)\n\nPrint(GetMemUsage())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize model configurations","metadata":{}},{"cell_type":"code","source":"%%time \n\n# Initializing Models \n\nif CFG.ML == \"Y\":\n    Mdl_Master = \\\n    {\n        \"CBR\" : CBR(**{'task_type' : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n                      'objective' : \"MAE\",\n                      'eval_metric' : 'MAE',\n                      'bagging_temperature' : 0.5,\n                      'colsample_bylevel' : 0.7,\n                      'iterations' : 500,\n                      'learning_rate' : 0.065,\n                      'od_wait' : 25,\n                      'max_depth' : 7,\n                       '12_leaf_reg' : 1.5,\n                       'min_data_in_leaf' : 1000,\n                       'random_strength' : 0.65,\n                       'verbose' : 0,\n                       'use_best_model' : True,\n                      }),\n        'LGBMR' : LGBMR(**{'device' : 'gpu' if CFG.gpu_switch == \"ON\" else \"cpu\",\n                           'objective' : 'regression_l1',\n                           'boosting_type' : 'gbdt',\n                           'random_state' : CFG.state,\n                           'colsample_bytree' : 0.7,\n                           'subsample' : 0.65,\n                           'learning_rate' : 0.065,\n                           'max_depth' : 6,\n                           'n_estimators' : 500,\n                           'num_leaves' : 150,\n                           'reg_alpha' : 0.01,\n                           'reg_lambda' : 3.25,\n                           'verbose' : -1,\n            \n                        }),\n        'XGBR' : XGBR(**{'tree_method' : 'gpu_hist' if CFG.gpu_switch == 'ON' else 'hist',\n                         'objective' : 'reg:absoluteerror',\n                         'random_state' : CFG.state,\n                         'colsample_bytree' : 0.7,\n                         'learning_rate' : 0.07,\n                         'max_depth' : 6,\n                         'n_estimators' : 500,\n                         'reg_alpha' : 0.025,\n                         'reg_lambda' : 1.75,\n                         'min_child_weight' : 1000,\n                         'early_stopping_rounds' : CFG.nbrnd_erly_stp\n                        }),\n        \"HGBR\" : HGBR( loss = 'squared_error',\n                     learning_rate = 0.075,\n                     early_stopping = True,\n                     max_iter = 200,\n                     max_depth = 6,\n                     min_samples_leaf = 1500,\n                     12_regularization = 1.75,\n                     scoring = myscorer,\n                     random_state = CFG.state)\n    }\n    \ncollect()\nprint(GetMemUsage())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nif CFG.ML == \"Y\":\n    # Initialize the models from configuration class:\n    methods = CFG.methods\n    \n    # Initialize a folder to store the trained and fitted models\n    system('mkdir BaselineML')\n    \n    # Init the model path for storage \n    model_path = CFG.mdl_path\n    \n    # Initializing the CV object:\n    cv = all_cv[CFG.mdlcv_mthd]\n    \n    # Initializing score dataframe \n    Scores = pd.Dataframe(index = range(CFG.n_splits * CFG.n_repeats),\n                         columns = methods).fillna(0).astype(np.float32)\n    \n    FtreImp = pd.DataFrame(index = X.columns, columns = [methods]).fillna(0)\n    \ncollect()\nlibc.malloc_trim(0)\nprint(GetMemUsage())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.ML == \"Y\":\n    print(\"=\"*25, \"Trainig\", \"-\" 25)\n    \n    # Initializing CV splitting \n    for fold_nb, (train_idx, val_idx) in tddm(enumerate(cv.split(X,y)),\n                                             f\"{CFG.mdlcv_mthd} CV {CFG.n_splits} * {CFG.n_repeats}\"):\n        # creating the CV folds\n        X_train = X.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_train = y.iloc[train_idx]\n        y_val = y.iloc[val_idx]\n        \n        print(f\"-------------- Fold {fold_nb} ------------------\")\n        \n        # Fitting the models \n        for method in methods:\n            model = Mdl_Master[method]\n            if method == \"LGBMR\":\n                model.fit(X_train, y_train,\n                         eval_set = [(X_val, y_val)],\n                         verbose = 0,\n                         eval_metric = 'mae',\n                         callbacks = [log_evaluation(0,),\n                                     early_stopping(CFG.nbrnd_erly_stp, verbose = False)])\n            elif method == \"XGBR\":\n                model.fit(X_train, y_train,\n                         eval_set = [(X_val,y_val)],\n                         verbose = 0,\n                         eval_metric = 'mae')\n            elif method == \"CBR\":\n                model.fit(X_train, y_train,\n                         eval_set = [(X_val, y_val)],\n                         verbose = 0,\n                         early_stopping_rounds = CFG.nbrnd_erly_stp)\n            else:\n                model.fit(X_train, y_train)\n                \n            joblib.dump(model, CFG.mdl_oath + f\"{method}v{CFG.version_nb}Fold{fold_nb}.model\")\n            \n            # Creating OOF scores\n            score = ScoreMetric(y_val, model,predict(X_val))\n            scores.at[fold_nb, method] = score\n            num_space = 6 - len(method)\n            print(f\"----- {method} {' ' * num_space} OOF = {score:.5f} --------\")\n            \n            del num_space, score\n            \n            # Collecting Feature importance \n            try: \n                FtreImp[method] = FtreImp[method].values + (model.feature_importances_ / (CFG.n_splits * CFG.n_repeats))\n            except:\n                pass\n            \n            collect()\n            \n        del X_train, y_train, X_val, y_val\n        collect()\n            \n    clear_output()\n    print(\" Mean OOF Scores across methods\")\n    display(Scores.mean())\n    \n    try: FtreImp.to_csv(CFG.mdl_path + f\"FtreImp_V{CFG.version_nb}.csv\")\n    except: pass\n    \ncollect()\nlibc.malloc_trim(0)\nprint(GetMemUsage())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\ndef MakeFeature(df: pd.DataFrame, prices: list) -> pd.DataFrame:\n    \"\"\"\n    The function creates new features using the prices columns. \n    \n    ------------------------------------------------------------\n    Parameters\n    ----------\n    df: pd.DataFrame\n    prices: prices columns for transformation\n    \n    -----------------------------------------------------------\n    Returns\n    --------\n    df: pd.DataFrame\n        DataFrame with addtional columns after feature engineering\n    \"\"\"\n    \n    features = ['overall_medvol', \"first5min_medvol\", \"last5min_medvol\",\n                'seconds_in_bucket', 'imbalance_buy_sell_flag',\n                'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n                'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n                'imb_s1', 'imb_s2']\n    df['imb_s1'] = df.eval('(bid_size - ask_size) / (bid_size + ask_size)').astype(np.float32)\n    df['imb_s2'] = df.eval('(imbalance_size - matched_size) / (matched_size + imbalance_sizes)').astype(np.float32)\n    \n    for i,a in enumerate(prices):\n        for j,b in enumerate(prices):\n            if i > j:\n                df[f'{a}_{b}_imb'] = df.eval(f\"({a} - {b}) / ({a} + {b})\")\n                features.append(f'{a}_{b}_imb')\n    for i,a in enumerate(prices):\n        for j,b in enumerate(prices):\n            for k,c enumerate(prices):\n                 if i>j and j>k:\n                    max_ = df[[a,b,c]].max(axis=1);\n                    min_ = df[[a,b,c]].min(axis=1);\n                    mid_ = df[[a,b,c]].sum(axis=1)-min_-max_\n\n                    df[f'{a}_{b}_{c}_imb2'] = ((max_-mid_)/(mid_-min_)).astype(np.float32);\n                    features.append(f'{a}_{b}_{c}_imb2')\n    \n    return df[features]\n\ncollect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Creating the test envir\n\nif CFG.inference_req == \"Y\":\n    try:\n        del X,y\n    except:\n        pass\n    \n    prices = ['reference_price', 'far_price','near_price',\n             'bid_price','ask_price','wap']\n    \n    # Making the test envir for inference\n    \n    import optiver2023\n    try:\n        env = optiver2023.make_env()\n        iter_test = env.iter_test()\n        print(\"Curating the ingerence environment\")\n    except:\n        pass\n    \n    # collating list of models \n    \n    models = []\n    \n    # loading the models for inference \n    \n    if CFG.ML != \"Y\":\n        model_path = CFG.inf_path\n        print(f\"Loading models from the input data for the kernel -V{CFG.version_nb}\")\n    elif CFG.ML == \"Y\":\n        model_path = CFG.mdl_path\n        print(\"Loading models from the working directory for the kernel\")\n        \n    model_label = []\n    for _, _, filename in walk(model_path):\n        model_label.extend(filename)\n    \n    models = []\n    for filename in model_label:\n        models.append(joblib.load(model_path+f\"{filename}\"))\n        \n    model_label = [m.replace(r\".model\",\"\") for m in model_label]\n    \n    model_dict = {l:m for l,m in zip(model_label, models)}\n    print(\"Trained Models\")\n    print(np.array(model_label))\n\ncollect()\nlibc.malloc_trim(0)\nprint(GetMemUsage())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}