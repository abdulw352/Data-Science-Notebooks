{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport time \nimport argparse\nfrom dataclasses import dataclass\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass ModelConfig:\n    \n    block_size: int = None # length of the input sequences of integers\n    vocab_size: int = None # the input integers are in range [0 .. vocab_size - 1]\n    # parameters below control the size of each model slightly differently \n    \n    n_layers: int = 4\n    n_embd: int = 64\n    n_embd2: int = 64\n    n_head: int = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transformer Language Model as used in GPT-2","metadata":{}},{"cell_type":"code","source":"class NewGELU(nn.Module):\n    \n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Gaussian Error Linear Units (GELU):    https://arxiv.org/abs/1606.08415\n    \"\"\"\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0/ math.pi ) + (x + 0.044715 * torch.pow(x, 3.0)) ))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    \"\"\"\n    A simple multi-head masked self-attention layer with a projection at the end. \n    \n    Similar to torch.nn.MultiheadAttention\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projection for all head, but in a batch\n        self.c_attn == nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1, config.block_size, config.block_size ))\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        \n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        \n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim.\n        q, k , v = self.c_attn(x).split(self.n_embd, dim = 2)\n        k = k.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        \n        # causal self.attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B ,nh ,T ,hs) -> (B, nh, T, hs)\n        y = y.transpose(1,2).contiguous().view(B,T,C) # re-assemble will head outputs side by side\n        \n        # output projection\n        y = self.c_proj(y)\n        return y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\" Unassuming Transformer Block \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n            act = NewGELU(),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x)))\n        \n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    \"\"\" Transformer Language Model, similar to GPT-2 \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.black_size = config.block_size\n        \n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n        \n        # Report number of parameters (note we don't count the decoder parameters in lm_head)\n        n_params = sum(p.numel() for p in self.transformer.paramters())\n        print(f\"number of paramters: {n_params/1e6 : .2f}\")\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1,t)\n        \n        # forward the GPT model \n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = tok_emb + pos_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        \n        # If we are given some desired targets so calculate the loss\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bag of Words (BOW) language Model","metadata":{}},{"cell_type":"code","source":"class CausalBoW(nn.Module):\n    \"\"\"\n    Causal bag of words. Averages the preceding elements and looks suspiciously like a CausalAttention module found in a transformer.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        \n        \n        # used to mask out vectors and preserve autoregressive property\n        self.block_size = config.block_size\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(t,\n                                                                                                      config.block_size, config.block_size))\n        \n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, n_embd\n        \n        # do the weighted average of all preceeding token features\n        att = torch.zeroes((B,T,T), device = x.device)\n        att = att.masked_fill(self.bias[:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim = -1)\n        y = att @ x # (B,T,T) x (B, T, C) -> (B,T,C)\n        \n        return y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BowBlock(nn.Module):\n    \"\"\"\n    collects BoW features and adds an MLP\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        # Causal BoW module\n        self.cbow = CausalBoW(config)\n        # MLP assembler\n        self.mlp = nn.ModuleDict(dict(\n            c_fc = nn.Linear(config.n_embd, config.n_embd2),\n          c_proj = nn.Linear(config.n_embd2, config.n_embd),\n        ))\n        \n        m = self.mlp\n        self.mlpf = lambda x: m.c_proj(F.tanh(m.c_fc(x))) # MLP forward\n        \n    def forward(self, x):\n        x = x + self.cbow(x)\n        x = x + self.mlpf(x)\n        return x\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BoW(nn.Module):\n    \n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n        self.vocab_size = config.vocab_size\n        # token embedding\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        # position embedding\n        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n        # context block\n        self.context_block = BoWBlock(config)\n        # language model head decoder layer\n        self.lm_head = nn.Linear(config.n_embd, self.vocab_size)\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        \n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype = torch.long, device = device).unsqueeze(0) # shape (1, t)\n        \n        # forward the token and position embedding layers\n        tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.wpe(idx) # position embeddings of shape (1, t, n_embd)\n        # add and run through the decoder MLP\n        x = tok_emb + pos_emb\n        # run the bag of words context module\n        x = self.context_block(x)\n        # decode to next token_probability\n        logits = self.lm_head(x)\n        \n        # if target is given\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recurrent Neural Net Language Model:\nGRU implemented since similar to LSTM easier to implement and works just as well","metadata":{}},{"cell_type":"code","source":"class RNNCell(nn.Module):\n    \"\"\"\n    The job of the 'cell' is take the input at current time step x_(t) and the hidden state at the previous time step h_(t-1) and return the resulting \n    hidden state h_{t} at the current time step.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2 )\n        \n    def forward(self, xt, hprev):\n        xh = torch.cat([xt, hprev], dim=1)\n        ht = F.tanh(self.xh_to_h(xh))\n        return ht","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GRUcell(nn.Module):\n    \"\"\"\n    similar to RNNCell but with a recurrence formula that makes the GRU more expressive and easier to optimism.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # Input, forget, output, gate\n        self.xh_to_z = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n        self.xh_to_r = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n        self.xh_to_hbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n        \n    def forward(self, xt, hprev):\n        # First use the reset fate to wipe some channels of the hidden state to zero.\n        xh = torch.cat([xt, hprev], dim = 1)\n        r = F.sigmoid(self.xh_to_r(xh))\n        hprev_reset = r * hprev\n        # calculate the candidate new hidden state hbar\n        xhr = torch.cat([xt, hprev_reset], dim = 1)\n        hbar = F.tanh(self.ch_to_hbar(xhr))\n        # calculate the switch gate that determines if each channel should be updated at all \n        z = F.sigmoid(self.xh_to_x(xh))\n        # blend the previous hidden state and the new candidate hidden state\n        ht = (1 - z) * hprev + z * hbar\n        return ht","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    \n    def __init__(self, config, cell_type):\n        super().__init__()\n        self.block_size = config.block_size\n        self.vocab_size = config.vocab_size\n        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state \n        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embedding table\n        if cell_tpye == 'rnn':\n            self.cell = RNNCell(config)\n        elif cell_type == 'gru':\n            self.cell = GRUCell(config)\n        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        device = idx.device\n        b, t = idx.size()\n        \n        # embed all the integers up front and all at once for efficiency\n        emb = self.wte(idx) # (b, t, n_embd)\n        \n        # sequentially iterate over the inputs and update the RNN state each tick\n        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n        hiddens = []\n        for i in range(t):\n            xt = emb[:, 1, :] # (b, n_embd)\n            ht = self.cell(xt, hprev) # ( b, n_embd2)\n            hprev = ht\n            hiddens.append(ht)\n            \n        # decode the outputs\n        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n        logits = self.lm_head(hidden)\n        \n        # if target is given\n        loss = None\n        if targers is not None:\n            loss = F.cross_entropy(logist.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1)\n            \n        return logits, loss\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MLP Language model","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Module):\n    \"\"\"\n    takes the previous block_size, tokens encodes them with a lookup table,\n    concatenates the vectors and predicts the next token with a MLP.\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n        self.vocab_size = config.vocab_size\n        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table\n        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token before the beginning of the input sequence\n        self.mlp = nn.Sequential(\n            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n            nn.Tanh(),\n            nn.Linear(config.n_embd2, self.vocab_size)\n        )\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        # gather the word embeddings of the previous 3 words\n        embs = []\n        for k in range(self.block_size):\n            toke_emb = self.wte(idx) # token embeddings of shape (b,t, n_embd)\n            idx = torch.roll(idx, 1, 1)\n            idx[:, 0] = self.vocab_size # special <BLANK> token\n            embs.append(tok_emb)\n            \n        # concat all of the embeddings together and pass through a MLP\n        x = torch.cat(embs, -1) # (, t, n_embd * block_size)\n        logits = self.mlp(x)\n        \n        # if given targets\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.view(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bigram Language Model","metadata":{}},{"cell_type":"code","source":"class Bigram(nn.Module):\n    \"\"\"\n    Bigram Language Model ' neural net', essentially a lookup table of logits for the next character\n    given a previous character.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        n = config.vocab_size\n        self.logits = nn.Paramater(torch.zero(n,n))\n        \n    def get_block_size(self):\n        return 1 # only predicting the next character based on the 1 previous char\n    \n    def forward(self, idx, targets = None):\n        # 'forward pass'\n        logits = self.logits[idx]\n        \n        # if we are given targets\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Helper Functions","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef generate(model, idx, max_new_tokens, temperature = 1.0, do_sample = False, top_k = None):\n    \"\"\"\n    Take a conditioning sequence of indices idx (Tensor of shape(b,t)) and complete the sequence \n    `max_new_tokens` times, feeding the prediction back into the model each time, \n    Best to be in Model.eval() mode\n    \"\"\"\n    \n    block_size = model.get_block_size()\n    for _ in range(max_new_tokens):\n        # if the sequence context is growing too long we must crop it at block_size\n        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n        # forward the model to get the logits for the index in the sequence \n        logits, _ = model(idx_cond)\n        # pluck the logits at the final step and scale by desired temperature\n        logits = logits[:, -1, :] / temperature\n        # optionally cropt the logits to only the top k options\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float('Inf')\n        # apply softmax to convert logits to (normalized) probabilities \n        probs = F.softmax(logits, dim = -1)\n        # either sample from the distribution or take the most likely element\n        if do_sample:\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            _, idx_next = torch.topk(probs, num_samples=1)\n        # append sampled index to the running sequence and continue \n        Idx = torch.cat((idx, idx_next), dim=1)\n        \n    return idx\n\ndef print_samples(num=10):\n    \"\"\" samples mfrom the model and decoded samples \"\"\"\n    X_init = torch.zeros(num, 1, dtype=torch.long).to(args.device)\n    top_k = args.top_k if args.top_k != -1 else None\n    steps = train_dataset.get_output_length() - 1 # -1 because we already start with <START> token (index 0)\n    X_samp = generate(model, X_init, steps, top_k = top_k, do_sample = True).to('cpu')\n    train_samples, test_samples, new_samples = [], [], []\n    for i in range(X_samp.size(0)):\n        # get the i'th row of sampled integers, as ptyhon list\n        row = X_samp[1, 1:].tolist() # Cropping out the first <START> token\n        # token 0 is the <STOP> token, so we crop the output the sequence at that point\n        crop_index = row.index(0) if 0 in row else len(row)\n        row = row[:crop_index]\n        word_samp = train_dataset.decode(row)\n        # separately track samples that we have and not seen before\n        if trian_dataset.contrains(word_samp):\n            train_samples.append(word_samp)\n        elif test_dataset.contains(word_samp):\n            test_samples.append(word_samp)\n        else:\n            new_samples.append(word_samp)\n            \n    print('-'* 80)\n    for lst, desc in [(train_samples, 'in train'), (test_samples, 'in text'), (new_samples, 'new')]:\n        print(f\"{len(lst)} samples that are {desc}:\")\n        for word in lst:\n            print(word)\n    print('-' * 80)\n    \n@torch.inference_mode()\ndef evaluate(model, dataset, batch_size = 50, max_batches=None):\n    model.eval()\n    loader = DataLoader(dataset, shuffle = True, batch_size = batch_size, num_workers = 0)\n    losses = []\n    for i, batch in enumerate(loader):\n        batch = [t.to(args.device) for t in batch]\n        X, Y = batch\n        logits, loss = model(X,Y)\n        losses.append(loss.item())\n        if max_batches is not None and i >= max_batches:\n            break\n    mean_loss = torch.tensor(losses).mean().item()\n    model.train() # reset model back to training mode\n    return mean_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Helper functions for creating training and test datasets to omit words","metadata":{}},{"cell_type":"code","source":"class CharDataset(Dataset):\n    \n    def __init__(self, words, chars, max_word_length):\n        self.words = words\n        self.chars = chars\n        self.max_word_length = max_word_length\n        self.stoi = {ch: i+1 for i, ch in enumerate(chars)}\n        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n        \n    def __len__(self):\n        return len(self.words)\n    \n    def contrains(self, word):\n        return word in self.words\n    \n    def get_vocab_size(self):\n        return len(self.chars) + 1 # all the possible characters and special 0 token\n    \n    def get_output_length(self):\n        return self.max_word_length + 1 # <START> token followed by words\n    \n    def encode(self, word):\n        ix = torch.tensor([self.stoi[w] for w in word], dtype = torch.long)\n        return ix\n    \n    def decode(self, ix):\n        word = ''.join(self.itos[i] for i in ix)\n        return word\n    \n    def __getitem__(self, idx):\n        word = self.words[idx]\n        ix = self.encode(word)\n        x = torch.zeros(self.max_word_length + 1, dtype = torch.long)\n        y = torch.zeros(self.max_word_length + 1, dtype = torch.long)\n        x[1:1+len(ix)] - ix\n        y[:len(ix)] - ix\n        y[len(ix) + 1:] = -1 # index -1 will mask the loss at the inactive locations\n        return x, y\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}