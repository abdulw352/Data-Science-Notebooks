{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport time \nimport argparse\nfrom dataclasses import dataclass\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass ModelConfig:\n    \n    block_size: int = None # length of the input sequences of integers\n    vocab_size: int = None # the input integers are in range [0 .. vocab_size - 1]\n    # parameters below control the size of each model slightly differently \n    \n    n_layers: int = 4\n    n_embd: int = 64\n    n_embd2: int = 64\n    n_head: int = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transformer Language Model as used in GPT-2","metadata":{}},{"cell_type":"code","source":"class NewGELU(nn.Module):\n    \n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Gaussian Error Linear Units (GELU):    https://arxiv.org/abs/1606.08415\n    \"\"\"\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0/ math.pi ) + (x + 0.044715 * torch.pow(x, 3.0)) ))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    \"\"\"\n    A simple multi-head masked self-attention layer with a projection at the end. \n    \n    Similar to torch.nn.MultiheadAttention\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projection for all head, but in a batch\n        self.c_attn == nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1, config.block_size, config.block_size ))\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        \n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        \n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim.\n        q, k , v = self.c_attn(x).split(self.n_embd, dim = 2)\n        k = k.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        \n        # causal self.attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B ,nh ,T ,hs) -> (B, nh, T, hs)\n        y = y.transpose(1,2).contiguous().view(B,T,C) # re-assemble will head outputs side by side\n        \n        # output projection\n        y = self.c_proj(y)\n        return y","metadata":{},"execution_count":null,"outputs":[]}]}