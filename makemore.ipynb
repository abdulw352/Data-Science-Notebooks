{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport time \nimport argparse\nfrom dataclasses import dataclass\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass ModelConfig:\n    \n    block_size: int = None # length of the input sequences of integers\n    vocab_size: int = None # the input integers are in range [0 .. vocab_size - 1]\n    # parameters below control the size of each model slightly differently \n    \n    n_layers: int = 4\n    n_embd: int = 64\n    n_embd2: int = 64\n    n_head: int = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transformer Language Model as used in GPT-2","metadata":{}},{"cell_type":"code","source":"class NewGELU(nn.Module):\n    \n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Gaussian Error Linear Units (GELU):    https://arxiv.org/abs/1606.08415\n    \"\"\"\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0/ math.pi ) + (x + 0.044715 * torch.pow(x, 3.0)) ))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    \"\"\"\n    A simple multi-head masked self-attention layer with a projection at the end. \n    \n    Similar to torch.nn.MultiheadAttention\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projection for all head, but in a batch\n        self.c_attn == nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1, config.block_size, config.block_size ))\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        \n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        \n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim.\n        q, k , v = self.c_attn(x).split(self.n_embd, dim = 2)\n        k = k.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        \n        # causal self.attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B ,nh ,T ,hs) -> (B, nh, T, hs)\n        y = y.transpose(1,2).contiguous().view(B,T,C) # re-assemble will head outputs side by side\n        \n        # output projection\n        y = self.c_proj(y)\n        return y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\" Unassuming Transformer Block \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n            act = NewGELU(),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x)))\n        \n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    \"\"\" Transformer Language Model, similar to GPT-2 \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.black_size = config.block_size\n        \n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n        \n        # Report number of parameters (note we don't count the decoder parameters in lm_head)\n        n_params = sum(p.numel() for p in self.transformer.paramters())\n        print(f\"number of paramters: {n_params/1e6 : .2f}\")\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1,t)\n        \n        # forward the GPT model \n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = tok_emb + pos_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        \n        # If we are given some desired targets so calculate the loss\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bag of Words (BOW) language Model","metadata":{}},{"cell_type":"code","source":"class CausalBoW(nn.Module):\n    \"\"\"\n    Causal bag of words. Averages the preceding elements and looks suspiciously like a CausalAttention module found in a transformer.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        \n        \n        # used to mask out vectors and preserve autoregressive property\n        self.block_size = config.block_size\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(t,\n                                                                                                      config.block_size, config.block_size))\n        \n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, n_embd\n        \n        # do the weighted average of all preceeding token features\n        att = torch.zeroes((B,T,T), device = x.device)\n        att = att.masked_fill(self.bias[:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim = -1)\n        y = att @ x # (B,T,T) x (B, T, C) -> (B,T,C)\n        \n        return y","metadata":{},"execution_count":null,"outputs":[]}]}