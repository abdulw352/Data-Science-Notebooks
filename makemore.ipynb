{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport time \nimport argparse\nfrom dataclasses import dataclass\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass ModelConfig:\n    \n    block_size: int = None # length of the input sequences of integers\n    vocab_size: int = None # the input integers are in range [0 .. vocab_size - 1]\n    # parameters below control the size of each model slightly differently \n    \n    n_layers: int = 4\n    n_embd: int = 64\n    n_embd2: int = 64\n    n_head: int = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transformer Language Model as used in GPT-2","metadata":{}},{"cell_type":"code","source":"class NewGELU(nn.Module):\n    \n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Gaussian Error Linear Units (GELU):    https://arxiv.org/abs/1606.08415\n    \"\"\"\n    \n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0/ math.pi ) + (x + 0.044715 * torch.pow(x, 3.0)) ))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n    \"\"\"\n    A simple multi-head masked self-attention layer with a projection at the end. \n    \n    Similar to torch.nn.MultiheadAttention\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projection for all head, but in a batch\n        self.c_attn == nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1, config.block_size, config.block_size ))\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        \n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        \n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim.\n        q, k , v = self.c_attn(x).split(self.n_embd, dim = 2)\n        k = k.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head ).transpose(1,2) # (B, nh, T, hs)\n        \n        # causal self.attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B ,nh ,T ,hs) -> (B, nh, T, hs)\n        y = y.transpose(1,2).contiguous().view(B,T,C) # re-assemble will head outputs side by side\n        \n        # output projection\n        y = self.c_proj(y)\n        return y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\" Unassuming Transformer Block \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n            act = NewGELU(),\n        ))\n        m = self.mlp\n        self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x)))\n        \n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    \"\"\" Transformer Language Model, similar to GPT-2 \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.black_size = config.block_size\n        \n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n        \n        # Report number of parameters (note we don't count the decoder parameters in lm_head)\n        n_params = sum(p.numel() for p in self.transformer.paramters())\n        print(f\"number of paramters: {n_params/1e6 : .2f}\")\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1,t)\n        \n        # forward the GPT model \n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = tok_emb + pos_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        \n        # If we are given some desired targets so calculate the loss\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bag of Words (BOW) language Model","metadata":{}},{"cell_type":"code","source":"class CausalBoW(nn.Module):\n    \"\"\"\n    Causal bag of words. Averages the preceding elements and looks suspiciously like a CausalAttention module found in a transformer.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        \n        \n        # used to mask out vectors and preserve autoregressive property\n        self.block_size = config.block_size\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(t,\n                                                                                                      config.block_size, config.block_size))\n        \n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, n_embd\n        \n        # do the weighted average of all preceeding token features\n        att = torch.zeroes((B,T,T), device = x.device)\n        att = att.masked_fill(self.bias[:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim = -1)\n        y = att @ x # (B,T,T) x (B, T, C) -> (B,T,C)\n        \n        return y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BowBlock(nn.Module):\n    \"\"\"\n    collects BoW features and adds an MLP\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        # Causal BoW module\n        self.cbow = CausalBoW(config)\n        # MLP assembler\n        self.mlp = nn.ModuleDict(dict(\n            c_fc = nn.Linear(config.n_embd, config.n_embd2),\n          c_proj = nn.Linear(config.n_embd2, config.n_embd),\n        ))\n        \n        m = self.mlp\n        self.mlpf = lambda x: m.c_proj(F.tanh(m.c_fc(x))) # MLP forward\n        \n    def forward(self, x):\n        x = x + self.cbow(x)\n        x = x + self.mlpf(x)\n        return x\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BoW(nn.Module):\n    \n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n        self.vocab_size = config.vocab_size\n        # token embedding\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        # position embedding\n        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n        # context block\n        self.context_block = BoWBlock(config)\n        # language model head decoder layer\n        self.lm_head = nn.Linear(config.n_embd, self.vocab_size)\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        \n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n        pos = torch.arange(0, t, dtype = torch.long, device = device).unsqueeze(0) # shape (1, t)\n        \n        # forward the token and position embedding layers\n        tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.wpe(idx) # position embeddings of shape (1, t, n_embd)\n        # add and run through the decoder MLP\n        x = tok_emb + pos_emb\n        # run the bag of words context module\n        x = self.context_block(x)\n        # decode to next token_probability\n        logits = self.lm_head(x)\n        \n        # if target is given\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recurrent Neural Net Language Model:\nGRU implemented since similar to LSTM easier to implement and works just as well","metadata":{}},{"cell_type":"code","source":"class RNNCell(nn.Module):\n    \"\"\"\n    The job of the 'cell' is take the input at current time step x_(t) and the hidden state at the previous time step h_(t-1) and return the resulting \n    hidden state h_{t} at the current time step.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2 )\n        \n    def forward(self, xt, hprev):\n        xh = torch.cat([xt, hprev], dim=1)\n        ht = F.tanh(self.xh_to_h(xh))\n        return ht","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GRUcell(nn.Module):\n    \"\"\"\n    similar to RNNCell but with a recurrence formula that makes the GRU more expressive and easier to optimism.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # Input, forget, output, gate\n        self.xh_to_z = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n        self.xh_to_r = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n        self.xh_to_hbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n        \n    def forward(self, xt, hprev):\n        # First use the reset fate to wipe some channels of the hidden state to zero.\n        xh = torch.cat([xt, hprev], dim = 1)\n        r = F.sigmoid(self.xh_to_r(xh))\n        hprev_reset = r * hprev\n        # calculate the candidate new hidden state hbar\n        xhr = torch.cat([xt, hprev_reset], dim = 1)\n        hbar = F.tanh(self.ch_to_hbar(xhr))\n        # calculate the switch gate that determines if each channel should be updated at all \n        z = F.sigmoid(self.xh_to_x(xh))\n        # blend the previous hidden state and the new candidate hidden state\n        ht = (1 - z) * hprev + z * hbar\n        return ht","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    \n    def __init__(self, config, cell_type):\n        super().__init__()\n        self.block_size = config.block_size\n        self.vocab_size = config.vocab_size\n        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state \n        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embedding table\n        if cell_tpye == 'rnn':\n            self.cell = RNNCell(config)\n        elif cell_type == 'gru':\n            self.cell = GRUCell(config)\n        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        device = idx.device\n        b, t = idx.size()\n        \n        # embed all the integers up front and all at once for efficiency\n        emb = self.wte(idx) # (b, t, n_embd)\n        \n        # sequentially iterate over the inputs and update the RNN state each tick\n        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n        hiddens = []\n        for i in range(t):\n            xt = emb[:, 1, :] # (b, n_embd)\n            ht = self.cell(xt, hprev) # ( b, n_embd2)\n            hprev = ht\n            hiddens.append(ht)\n            \n        # decode the outputs\n        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n        logits = self.lm_head(hidden)\n        \n        # if target is given\n        loss = None\n        if targers is not None:\n            loss = F.cross_entropy(logist.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1)\n            \n        return logits, loss\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MLP Language model","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Module):\n    \"\"\"\n    takes the previous block_size, tokens encodes them with a lookup table,\n    concatenates the vectors and predicts the next token with a MLP.\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n        self.vocab_size = config.vocab_size\n        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table\n        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token before the beginning of the input sequence\n        self.mlp = nn.Sequential(\n            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n            nn.Tanh(),\n            nn.Linear(config.n_embd2, self.vocab_size)\n        )\n        \n    def get_block_size(self):\n        return self.block_size\n    \n    def forward(self, idx, targets = None):\n        # gather the word embeddings of the previous 3 words\n        embs = []\n        for k in range(self.block_size):\n            toke_emb = self.wte(idx) # token embeddings of shape (b,t, n_embd)\n            idx = torch.roll(idx, 1, 1)\n            idx[:, 0] = self.vocab_size # special <BLANK> token\n            embs.append(tok_emb)\n            \n        # concat all of the embeddings together and pass through a MLP\n        x = torch.cat(embs, -1) # (, t, n_embd * block_size)\n        logits = self.mlp(x)\n        \n        # if given targets\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.view(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bigram Language Model","metadata":{}},{"cell_type":"code","source":"class Bigram(nn.Module):\n    \"\"\"\n    Bigram Language Model ' neural net', essentially a lookup table of logits for the next character\n    given a previous character.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        n = config.vocab_size\n        self.logits = nn.Paramater(torch.zero(n,n))\n        \n    def get_block_size(self):\n        return 1 # only predicting the next character based on the 1 previous char\n    \n    def forward(self, idx, targets = None):\n        # 'forward pass'\n        logits = self.logits[idx]\n        \n        # if we are given targets\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index = -1 )\n            \n        return logits, loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Helper Functions","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef generate(model, idx, max_new_tokens, temperature = 1.0, do_sample = False, top_k = None):\n    \"\"\"\n    Take a conditioning sequence of indices idx (Tensor of shape(b,t)) and complete the sequence \n    `max_new_tokens` times, feeding the prediction back into the model each time, \n    Best to be in Model.eval() mode\n    \"\"\"\n    \n    block_size = model.get_block_size()\n    for _ in range(max_new_tokens):\n        # if the sequence context is growing too long we must crop it at block_size\n        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n        # forward the model to get the logits for the index in the sequence \n        logits, _ = model(idx_cond)\n        # pluck the logits at the final step and scale by desired temperature\n        logits = logits[:, -1, :] / temperature\n        # optionally cropt the logits to only the top k options\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float('Inf')\n        # apply softmax to convert logits to (normalized) probabilities \n        probs = F.softmax(logits, dim = -1)\n        # either sample from the distribution or take the most likely element\n        if do_sample:\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            _, idx_next = torch.topk(probs, num_samples=1)\n        # append sampled index to the running sequence and continue \n        Idx = torch.cat((idx, idx_next), dim=1)\n        \n    return idx\n\ndef print_samples(num=10):\n    \"\"\" samples mfrom the model and decoded samples \"\"\"\n    X_init = torch.zeros(num, 1, dtype=torch.long).to(args.device)\n    top_k = args.top_k if args.top_k != -1 else None\n    steps = train_dataset.get_output_length() - 1 # -1 because we already start with <START> token (index 0)\n    X_samp = generate(model, X_init, steps, top_k = top_k, do_sample = True).to('cpu')\n    train_samples, test_samples, new_samples = [], [], []\n    for i in range(X_samp.size(0)):\n        # get the i'th row of sampled integers, as ptyhon list\n        row = X_samp[1, 1:].tolist() # Cropping out the first <START> token\n        # token 0 is the <STOP> token, so we crop the output the sequence at that point\n        crop_index = row.index(0) if 0 in row else len(row)\n        row = row[:crop_index]\n        word_samp = train_dataset.decode(row)\n        # separately track samples that we have and not seen before\n        if trian_dataset.contrains(word_samp):\n            train_samples.append(word_samp)\n        elif test_dataset.contains(word_samp):\n            test_samples.append(word_samp)\n        else:\n            new_samples.append(word_samp)\n            \n    print('-'* 80)\n    for lst, desc in [(train_samples, 'in train'), (test_samples, 'in text'), (new_samples, 'new')]:\n        print(f\"{len(lst)} samples that are {desc}:\")\n        for word in lst:\n            print(word)\n    print('-' * 80)\n    \n@torch.inference_mode()\ndef evaluate(model, dataset, batch_size = 50, max_batches=None):\n    model.eval()\n    loader = DataLoader(dataset, shuffle = True, batch_size = batch_size, num_workers = 0)\n    losses = []\n    for i, batch in enumerate(loader):\n        batch = [t.to(args.device) for t in batch]\n        X, Y = batch\n        logits, loss = model(X,Y)\n        losses.append(loss.item())\n        if max_batches is not None and i >= max_batches:\n            break\n    mean_loss = torch.tensor(losses).mean().item()\n    model.train() # reset model back to training mode\n    return mean_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Helper functions for creating training and test datasets to omit words","metadata":{}},{"cell_type":"code","source":"class CharDataset(Dataset):\n    \n    def __init__(self, words, chars, max_word_length):\n        self.words = words\n        self.chars = chars\n        self.max_word_length = max_word_length\n        self.stoi = {ch: i+1 for i, ch in enumerate(chars)}\n        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n        \n    def __len__(self):\n        return len(self.words)\n    \n    def contrains(self, word):\n        return word in self.words\n    \n    def get_vocab_size(self):\n        return len(self.chars) + 1 # all the possible characters and special 0 token\n    \n    def get_output_length(self):\n        return self.max_word_length + 1 # <START> token followed by words\n    \n    def encode(self, word):\n        ix = torch.tensor([self.stoi[w] for w in word], dtype = torch.long)\n        return ix\n    \n    def decode(self, ix):\n        word = ''.join(self.itos[i] for i in ix)\n        return word\n    \n    def __getitem__(self, idx):\n        word = self.words[idx]\n        ix = self.encode(word)\n        x = torch.zeros(self.max_word_length + 1, dtype = torch.long)\n        y = torch.zeros(self.max_word_length + 1, dtype = torch.long)\n        x[1:1+len(ix)] - ix\n        y[:len(ix)] - ix\n        y[len(ix) + 1:] = -1 # index -1 will mask the loss at the inactive locations\n        return x, y\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_datasets(input_file):\n    \n    # preprocessing of the input text file \n    with open(input_file, 'r') as f:\n        data = f.read()\n    words  data.splitlines()\n    words = [w.strip() for w in words] # get rid of any leading or trailing white space \n    words = [w for w in words if w] # get rid of any empty strings\n    chars = sorted(list(set(''.join(words)))) # all the possible characters\n    max_word_length = max(len(w) for w in words)\n    print(f\"number of examples in the dataset: {len(words)}\")\n    print(f\"max word length: {max_word_length}\")\n    print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n    print(\"vocabulary: \")\n    print(''.join(chars))\n    \n    # partition the input data into a training and the test set\n    test_set_size = min(1000, int(len(words) + 0.1)) # 10% of the training set, or up to 1000 examples\n    rp = torch.randperm(len(words)).tolist()\n    train_words = [words[i] for i in rp[:-test_set_size]]\n    test_words = [words[i] for i in rp[-test_set_size]]\n    print(f\"split up the dataset into {len(train_words)} training examples and {len(test_words)} test examples.\")\n    \n    # wrap in dataset objects\n    train_dataset = CharDataset(train_words, chars, max_word_length)\n    test_dataset = CharDataset(test_words, chars, max_word_length)\n    \n    return train_dataset, test_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InfiniteDataLoader:\n    \"\"\"\n    hacky way to create an infinte dataset since no way to do this with pytorch\n    \"\"\"\n    \n    def __init__(self, dataset, **kwargs):\n        train_sampler = torch.utils.RandomSampler(dataset, replacement = True, num_samples = int(1e10))\n        self.train_loader = DataLoader(dataset, sampler = train_sampler, **kwargs)\n        self.data_iter = iter(self.train_loader)\n        \n    def next(self):\n        try:\n            batch = next(self.data_iter)\n        except StopIteration: # this will happen only after 1e10 ... essentially never\n            self.data_iter = iter(self.train_loader)\n            batch = next(self.data_iter)\n        return batch\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# if __name__ == '__main__':","metadata":{}},{"cell_type":"code","source":"# parse command line args\nparser = argparse.ArgumentParser(description = \"MakeMore\")\n# system/input/output\npasser.add_argument('--input-file', '-i', type = str, default='names.txt', help = 'input file with things one per line.')\nparser.add_argument('--work-dir', '-o', type=str, default='out', help=\"output working directory\")\n    parser.add_argument('--resume', action='store_true', help=\"when this flag is used, we will resume optimization from existing model in the workdir\")\n    parser.add_argument('--sample-only', action='store_true', help=\"just sample from the model and quit, don't train\")\n    parser.add_argument('--num-workers', '-n', type=int, default=4, help=\"number of data workers for both train/test\")\n    parser.add_argument('--max-steps', type=int, default=-1, help=\"max number of optimization steps to run for, or -1 for infinite.\")\n    parser.add_argument('--device', type=str, default='cpu', help=\"device to use for compute, examples: cpu|cuda|cuda:2|mps\")\n    parser.add_argument('--seed', type=int, default=3407, help=\"seed\")\n    # sampling\n    parser.add_argument('--top-k', type=int, default=-1, help=\"top-k for sampling, -1 means no top-k\")\n    # model\n    parser.add_argument('--type', type=str, default='transformer', help=\"model class type to use, bigram|mlp|rnn|gru|bow|transformer\")\n    parser.add_argument('--n-layer', type=int, default=4, help=\"number of layers\")\n    parser.add_argument('--n-head', type=int, default=4, help=\"number of heads (in a transformer)\")\n    parser.add_argument('--n-embd', type=int, default=64, help=\"number of feature channels in the model\")\n    parser.add_argument('--n-embd2', type=int, default=64, help=\"number of feature channels elsewhere in the model\")\n    # optimization\n    parser.add_argument('--batch-size', '-b', type=int, default=32, help=\"batch size during optimization\")\n    parser.add_argument('--learning-rate', '-l', type=float, default=5e-4, help=\"learning rate\")\n    parser.add_argument('--weight-decay', '-w', type=float, default=0.01, help=\"weight decay\")\n    args = parser.parse_args()\n    print(vars(args))\n\n    # system inits\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    os.makedirs(args.work_dir, exist_ok=True)\n    writer = SummaryWriter(log_dir=args.work_dir)\n\n    # init datasets\n    train_dataset, test_dataset = create_datasets(args.input_file)\n    vocab_size = train_dataset.get_vocab_size()\n    block_size = train_dataset.get_output_length()\n    print(f\"dataset determined that: {vocab_size=}, {block_size=}\")\n\n    # init model\n    config = ModelConfig(vocab_size=vocab_size, block_size=block_size,\n                       n_layer=args.n_layer, n_head=args.n_head,\n                       n_embd=args.n_embd, n_embd2=args.n_embd2)\n    if args.type == 'transformer':\n        model = Transformer(config)\n    elif args.type == 'bigram':\n        model = Bigram(config)\n    elif args.type == 'mlp':\n        model = MLP(config)\n    elif args.type == 'rnn':\n        model = RNN(config, cell_type='rnn')\n    elif args.type == 'gru':\n        model = RNN(config, cell_type='gru')\n    elif args.type == 'bow':\n        model = BoW(config)\n    else:\n        raise ValueError(f'model type {args.type} is not recognized')\n    model.to(args.device)\n    print(f\"model #params: {sum(p.numel() for p in model.parameters())}\")\n    if args.resume or args.sample_only: # note: if we sample-only then we also assume we are resuming\n        print(\"resuming from existing model in the workdir\")\n        model.load_state_dict(torch.load(os.path.join(args.work_dir, 'model.pt')))\n    if args.sample_only:\n        print_samples(num=50)\n        sys.exit()\n\n    # init optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n\n    # init dataloader\n    batch_loader = InfiniteDataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, num_workers=args.num_workers)\n\n    # training loop\n    best_loss = None\n    step = 0\n    while True:\n\n        t0 = time.time()\n\n        # get the next batch, ship to device, and unpack it to input and target\n        batch = batch_loader.next()\n        batch = [t.to(args.device) for t in batch]\n        X, Y = batch\n\n        # feed into the model\n        logits, loss = model(X, Y)\n\n        # calculate the gradient, update the weights\n        model.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n        if args.device.startswith('cuda'):\n            torch.cuda.synchronize()\n        t1 = time.time()\n\n        # logging\n        if step % 10 == 0:\n            print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n\n        # evaluate the model\n        if step > 0 and step % 500 == 0:\n            train_loss = evaluate(model, train_dataset, batch_size=100, max_batches=10)\n            test_loss  = evaluate(model, test_dataset,  batch_size=100, max_batches=10)\n            writer.add_scalar(\"Loss/train\", train_loss, step)\n            writer.add_scalar(\"Loss/test\", test_loss, step)\n            writer.flush()\n            print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n            # save the model to disk if it has improved\n            if best_loss is None or test_loss < best_loss:\n                out_path = os.path.join(args.work_dir, \"model.pt\")\n                print(f\"test loss {test_loss} is the best so far, saving model to {out_path}\")\n                torch.save(model.state_dict(), out_path)\n                best_loss = test_loss\n\n        # sample from the model\n        if step > 0 and step % 200 == 0:\n            print_samples(num=10)\n\n        step += 1\n        # termination conditions\n        if args.max_steps >= 0 and step >= args.max_steps:\n            break","metadata":{},"execution_count":null,"outputs":[]}]}