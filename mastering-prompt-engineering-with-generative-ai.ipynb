{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploring Generative AI for Dialogue Summarization\n## Effective Prompt Engineering for Dialogue Summarization with Generative AI Google FLAN-T5-Base (Part 1)\nLet's delve into the practical side of our study. In this notebook, we'll be focusing on the dialogue summarization task using generative AI. Our objective is to observe how adjustments to the input text impact the model's output. Through prompt engineering, we'll guide the model to align with our specific task requirements. By experimenting with zero shot, one shot, and few shot inferences, we'll initiate the journey into prompt engineering, witnessing firsthand its influence on enhancing the generative output of Large Language Models.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Configuring Kernel and Installing Dependencies](#1)\n- [ 2 - Dialogue Summarization without Prompt Engineering](#2)\n- [ 3 - Summarizing Dialogue Using an Instruction Prompt](#3)\n  - [ 3.1 - Zero Shot Inference Using an Instruction Prompt](#3.1)\n  - [ 3.2 - Zero Shot Inference Using the FLAN-T5 Prompt Template](#3.2)\n- [ 4 - Summarizing Dialogue Using One Shot and Few Shot Inference](#4)\n  - [ 4.1 - One Shot Inference](#4.1)\n  - [ 4.2 - Few Shot Inference](#4.2)\n- [ 5 - Configuration Parameters for Generative Inference](#5)\n","metadata":{}},{"cell_type":"markdown","source":"# 1 - Configuring Kernel and Installing Dependencies\nLet's set up the kernel and install the necessary packages to leverage PyTorch, Hugging Face transformers, and datasets.\n\nNote: Executing this cell may require a few minutes.\n","metadata":{"tags":[]}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0  --quiet","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:29:19.303426Z","iopub.execute_input":"2023-10-14T22:29:19.303735Z","iopub.status.idle":"2023-10-14T22:33:15.379273Z","shell.execute_reply.started":"2023-10-14T22:29:19.303708Z","shell.execute_reply":"2023-10-14T22:33:15.377921Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-23.2.1\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\npathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\npathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load the datasets, Large Language Model (LLM), tokenizer, and configurator. Don't stress if you haven't grasped all these components yet; they'll be explained and discussed later in the notebook.","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:15.381568Z","iopub.execute_input":"2023-10-14T22:33:15.382238Z","iopub.status.idle":"2023-10-14T22:33:17.791592Z","shell.execute_reply.started":"2023-10-14T22:33:15.382197Z","shell.execute_reply":"2023-10-14T22:33:17.790689Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n# 2 - Dialogue Summarization without Prompt Engineering\n\nIn this scenario, the goal is to generate a summary of a dialogue using the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. You can explore the list of available models in the Hugging Face transformers package [here](https://huggingface.co/docs/transformers/index). \n\nLet's load some straightforward dialogues from the DialogSum Hugging Face dataset. This dataset comprises over 10,000 dialogues, each accompanied by manually labeled summaries and topics.","metadata":{}},{"cell_type":"markdown","source":"Load the dataset from Hugging Face. The dataset is already preprocessed and split into train, validation, and test sets. You will use the test set to evaluate the model performance.","metadata":{}},{"cell_type":"code","source":"huggingface_dataset_name = \"knkarthick/dialogsum\"\n\n# Load the dataset using Hugging Face's datasets library\ndataset = load_dataset(huggingface_dataset_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:17.793170Z","iopub.execute_input":"2023-10-14T22:33:17.793990Z","iopub.status.idle":"2023-10-14T22:33:21.597314Z","shell.execute_reply.started":"2023-10-14T22:33:17.793957Z","shell.execute_reply":"2023-10-14T22:33:21.596330Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecccea10aa2d4c0281d83f2e30e71773"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd6e5118e2241ffa6d85536eaf21ac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29eb5c89ceb34f319a08ec11f141d30a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16bda11e7e24da981c2813a1c1a0ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa9474a98994d5eaa9b2865add2e4bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3132e98225427aa22f93420f65207d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8b4a72b8da4db18813255dfbbff574"}},"metadata":{}}]},{"cell_type":"markdown","source":"Print a couple of dialogues with their baseline summaries.","metadata":{"tags":[]}},{"cell_type":"code","source":"example_indices = [40, 200]\n\ndash_line = '-'.join('' for x in range(100))\n\nfor i, index in enumerate(example_indices):\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print('INPUT DIALOGUE:')\n    print(dataset['test'][index]['dialogue'])\n    print(dash_line)\n    print('BASELINE HUMAN SUMMARY:')\n    print(dataset['test'][index]['summary'])\n    print(dash_line)\n    print()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:21.599605Z","iopub.execute_input":"2023-10-14T22:33:21.600382Z","iopub.status.idle":"2023-10-14T22:33:21.608904Z","shell.execute_reply.started":"2023-10-14T22:33:21.600349Z","shell.execute_reply":"2023-10-14T22:33:21.607673Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n---------------------------------------------------------------------------------------------------\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT DIALOGUE:\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load the pre-trained model FLAN-T5 from Hugging Face.\nIncorporate the FLAN-T5 model into our workflow, we'll begin by creating an instance of the `AutoModelForSeq2SeqLM` class using the `.from_pretrained()` method. This allows us to easily tap into the model's pre-trained capabilities.\n","metadata":{}},{"cell_type":"code","source":"# Specify the model name\nmodel_name='google/flan-t5-base'\n\n# Load the model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"id":"iAYlS40Z3l-v","tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:21.610178Z","iopub.execute_input":"2023-10-14T22:33:21.610763Z","iopub.status.idle":"2023-10-14T22:33:30.337479Z","shell.execute_reply.started":"2023-10-14T22:33:21.610731Z","shell.execute_reply":"2023-10-14T22:33:30.336648Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f72bb945e14f16bda39115e35f0b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f22f4730174c299c5ac88844dba618"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ccd95c8907409b87bfd57cc0ea34de"}},"metadata":{}}]},{"cell_type":"markdown","source":"To handle encoding and decoding, it's crucial to engage with text in a tokenized format. Tokenization is the practice of breaking down texts into smaller units, facilitating processing by LLM models.\n\nRetrieve the tokenizer for the FLAN-T5 model by employing the AutoTokenizer.from_pretrained() method. The use_fast parameter activates the fast tokenizer. Currently, we won't delve into the intricacies of this setting, but you can explore the tokenizer parameters further in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer).","metadata":{"id":"sPqQA3TT3l_I","tags":[]}},{"cell_type":"code","source":"# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"id":"sPqQA3TT3l_I","tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:30.338952Z","iopub.execute_input":"2023-10-14T22:33:30.339524Z","iopub.status.idle":"2023-10-14T22:33:31.998874Z","shell.execute_reply.started":"2023-10-14T22:33:30.339489Z","shell.execute_reply":"2023-10-14T22:33:31.997947Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b79afbab2c403fb11e7a3f64c61b5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27021b89f044ea9ac81d674225e3eea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8191d5aec0a446aeb13845e70ecaeb05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d576f54cb2b74d0aac56e918ab96d6cb"}},"metadata":{}}]},{"cell_type":"markdown","source":"Test the tokenizer encoding and decoding a simple sentence:","metadata":{"tags":[]}},{"cell_type":"code","source":"# Define a test sentence\nsentence = \"This is a test sentence.\"\n\n# Encode the sentence using the tokenizer, returning PyTorch tensors\nsentence_encoded = tokenizer(sentence, return_tensors='pt')\n\n# Decode the encoded sentence, skipping special tokens\nsentence_decoded = tokenizer.decode(\n        sentence_encoded[\"input_ids\"][0], \n        skip_special_tokens=True\n    )\n\n# Print the encoded sentence's representation\nprint('ENCODED SENTENCE:')\nprint(sentence_encoded[\"input_ids\"][0])\n\n# Print the decoded sentence\nprint('\\nDECODED SENTENCE:')\nprint(sentence_decoded)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-14T22:33:32.000481Z","iopub.execute_input":"2023-10-14T22:33:32.001038Z","iopub.status.idle":"2023-10-14T22:33:38.545805Z","shell.execute_reply.started":"2023-10-14T22:33:32.001002Z","shell.execute_reply":"2023-10-14T22:33:38.543331Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ENCODED SENTENCE:\ntensor([ 100,   19,    3,    9,  794, 7142,    5,    1])\n\nDECODED SENTENCE:\nThis is a test sentence.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's dive into assessing how effectively the base LLM summarizes a dialogue without incorporating any prompt engineering. \nIn simpler terms, **prompt engineering** involves humans tweaking the input to enhance the model's response for a specific task.","metadata":{}},{"cell_type":"code","source":"# Iterate through example indices (We defined them above), where each index represents a specific example\nfor i, index in enumerate(example_indices):\n\n    # Retrieve dialogue and summary for the current example\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n\n    # Tokenize the dialogue and convert it to a vector of PyTorch tensors\n    inputs = tokenizer(dialogue, return_tensors='pt')\n\n    # Generate an output using the model, limiting the new tokens to 50\n    # This uses the LLM to generate a summary of the dialogue without any prompt engineering\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n\n    # Show the results\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)\n    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-14T22:33:38.552378Z","iopub.execute_input":"2023-10-14T22:33:38.553365Z","iopub.status.idle":"2023-10-14T22:33:41.966672Z","shell.execute_reply.started":"2023-10-14T22:33:38.553280Z","shell.execute_reply":"2023-10-14T22:33:41.965628Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINEERING:\nPerson1: It's ten to nine.\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n#Person1#: I'm thinking of upgrading my computer.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model's guesses make some sense, but it seems unsure about the task. It's doesn't seem to understand that we want it to summarize the dialogue. It looks like it's just creating the next sentence in the dialogue without knowing exactly what it's supposed to do. \nLet's see if we can help it out a bit by providing some instructions on what we want it to do with the dialogue (Prompt Engineering).","metadata":{}},{"cell_type":"markdown","source":"<a name='3'></a>\n# 3 - Summarizing Dialogue Using an Instruction Prompt\n\nDigging into prompt engineering is crucial when working with foundational models for text generation. For a brief introduction to prompt engineering, you might find [this blog](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering) from Amazon Science interesting.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n## 3.1 - Zero Shot Inference Using an Instruction Prompt\n\nWhen you want to guide the model to perform a specific task, like summarizing a dialogue, one approach is to transform the dialogue into an instruction prompt. This technique is commonly known as **zero-shot inference**. For insights into what zero-shot learning is and why it's significant for LLM models, you might find **[this blog from AWS](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/)** helpful.\n\nSo here we will wrap the dialogue in a clear instruction and observe how the generated text responds:","metadata":{}},{"cell_type":"code","source":"# Iterate through example indices, where each index represents a specific example\nfor i, index in enumerate(example_indices):\n    # Retrieve dialogue and summary for the current example\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n\n    # Construct an instruction prompt for summarizing the dialogue \n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n    \"\"\"\n\n    # Tokenize the constructed prompt and convert it to PyTorch tensors\n    inputs = tokenizer(prompt, return_tensors='pt')\n    \n    # Generate an output using the model, limiting the new tokens to 50\n    # This uses the LLM to generate a summary of the dialogue with the constructed prompt\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n    \n    # Show the results\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)    \n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:41.968208Z","iopub.execute_input":"2023-10-14T22:33:41.968870Z","iopub.status.idle":"2023-10-14T22:33:44.272604Z","shell.execute_reply.started":"2023-10-14T22:33:41.968823Z","shell.execute_reply":"2023-10-14T22:33:44.271649Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nSummary:\n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nThe train is about to leave.\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nSummary:\n    \n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: I'm thinking of upgrading my computer.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This result shows improvement, but there's still room for enhancement. The model doesn't seem to capture the subtleties present in the conversations.","metadata":{}},{"cell_type":"markdown","source":"**Optional:**\n- Explore variations in the prompt text to observe changes in inferences. Test whether ending the prompt with an empty string versus Summary: affects the generated output.\n- Experiment with rephrasing the initial part of the prompt text from Summarize the following conversation. to something else, and observe its impact on the generated output.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.2'></a>\n## 3.2 - Zero Shot Inference Using the FLAN-T5 Prompt Template\n\nNow, let's switch things up a bit with a different prompt. FLAN-T5 offers various prompt templates tailored for specific tasks, and you can find them **[here](https://github.com/google-research/FLAN/tree/main/flan/v2)**. In the upcoming code, we'll employ one of the **[pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py)**:","metadata":{}},{"cell_type":"code","source":"# Iterate through example indices, where each index represents a specific example\nfor i, index in enumerate(example_indices):\n    # Retrieve dialogue and summary for the current example\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n    \n    # Construct a prompt for summarizing the dialogue using the FLAN-T5 template\n    prompt = f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n\n    # Tokenize the constructed prompt and convert it to PyTorch tensors\n    inputs = tokenizer(prompt, return_tensors='pt')\n    \n    # Generate an output using the model, limiting the new tokens to 50\n    # This uses the LLM to generate a summary of the dialogue with the constructed prompt\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n    \n    # Show the results\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n    print(dash_line)\n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:44.275876Z","iopub.execute_input":"2023-10-14T22:33:44.276166Z","iopub.status.idle":"2023-10-14T22:33:48.624766Z","shell.execute_reply.started":"2023-10-14T22:33:44.276138Z","shell.execute_reply":"2023-10-14T22:33:48.623789Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nExample  1\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nTom is late for the train.\n\n---------------------------------------------------------------------------------------------------\nExample  2\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what you will try to solve with the few shot inferencing.","metadata":{}},{"cell_type":"markdown","source":"<a name='4'></a>\n# 4 - Summarizing Dialogue Using One Shot and Few Shot Inference\n\nIn the realms of **one-shot and few-shot inference**, the approach involves presenting an LLM with either a single or a handful of complete examples of prompt-response pairs that align with your task. This practice, known as \"in-context learning,\" establishes a state in the model that comprehends the specifics of your task. You can delve deeper into this concept by reading **[this blog from HuggingFace](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)**.","metadata":{}},{"cell_type":"markdown","source":"<a name='4.1'></a>\n## 4.1 - One Shot Inference\n\nWe'll construct a function that accepts a list of **`example_indices_full`**, creates a prompt with full examples, and finally appends the prompt you want the model to complete (**`example_index_to_summarize`**). For this, we'll use the same FLAN-T5 prompt template from section [3.2](#3.2.). ","metadata":{"tags":[]}},{"cell_type":"code","source":"def make_prompt(full_examples_indices, index_to_summarize):\n    \"\"\"\n    Construct a prompt for one-shot or few-shot inference.\n\n    Parameters\n    ----------\n    full_examples_indices : list\n        A list containing indices for complete dialogues to be included in the prompt. These dialogues serve as examples \n        for the model to learn from (for one-shot or few-shot inference).\n    index_to_summarize : int\n        The index for the dialogue that the model is expected to give a summary for.\n\n    Returns\n    -------\n    str\n        A prompt string that is constructed as per the given parameters - full dialogues examples followed by a dialogue \n        that needs to be summarized.\n    \"\"\"\n    prompt = ''\n\n    # Go through each index in the full examples list\n    for index in full_examples_indices:\n        dialogue = dataset['test'][index]['dialogue']\n        summary = dataset['test'][index]['summary']\n\n        # Add each dialogue and its summary to the prompt string, followed by a stop sequence. The stop sequence \n        # '{summary}\\n\\n\\n' is essential for FLAN-T5 model. Other models may have their own different stop sequence.\n        prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n{summary}\n\n\n\"\"\"\n\n    # Now add the dialogue that needs to be summarized by the model\n    dialogue_to_summarize = dataset['test'][index_to_summarize]['dialogue']\n\n    # Append this new dialogue to the prompt string\n    prompt += f\"\"\"\nDialogue:\n\n{dialogue_to_summarize}\n\nWhat was going on?\n\"\"\"\n\n    # Return the constructed prompt\n    return prompt","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:48.626439Z","iopub.execute_input":"2023-10-14T22:33:48.627094Z","iopub.status.idle":"2023-10-14T22:33:48.633312Z","shell.execute_reply.started":"2023-10-14T22:33:48.627058Z","shell.execute_reply":"2023-10-14T22:33:48.632477Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Create the prompt for one-shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"# Define index for full example to be included in the prompt as a one-shot example\nfull_examples_indices = [40]\n# Define the index for the dialogue that the model is expected to give a summary for\nexample_index_to_summarize = 200\n\n# Create the prompt for one-shot inference\none_shot_prompt = make_prompt(full_examples_indices, example_index_to_summarize)\n\nprint(one_shot_prompt)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:48.634537Z","iopub.execute_input":"2023-10-14T22:33:48.635286Z","iopub.status.idle":"2023-10-14T22:33:48.652426Z","shell.execute_reply.started":"2023-10-14T22:33:48.635254Z","shell.execute_reply":"2023-10-14T22:33:48.651373Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, let's use this prompt for one-shot inference and observe the results (Generate a summary using the LLM with the prompt you just created):","metadata":{"tags":[]}},{"cell_type":"code","source":"# Retrieve the human-generated summary for the 'example_index_to_summarize' example\nsummary = dataset['test'][example_index_to_summarize]['summary']\n\n# Tokenize the one-shot prompt and convert it to PyTorch tensors\ninputs = tokenizer(one_shot_prompt, return_tensors='pt')\n\n# Generate an output using the model, limiting the new tokens to 50\n# This uses the LLM to generate a summary of the dialogue with the one-shot prompt\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\n# Show the results\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ONE SHOT:\\n{output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:48.653802Z","iopub.execute_input":"2023-10-14T22:33:48.654409Z","iopub.status.idle":"2023-10-14T22:33:52.116010Z","shell.execute_reply.started":"2023-10-14T22:33:48.654377Z","shell.execute_reply":"2023-10-14T22:33:52.115058Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ONE SHOT:\n#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='4.2'></a>\n## 4.2 - Few Shot Inference\n\nNow, let's explore few-shot inference by incorporating two additional full dialogue-summary pairs into our prompt.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Define indices for full examples to be included in the prompt as a few-shot examples \nfull_examples_indices = [40, 80, 120]\n# Define the index for the dialogue that the model is expected to give a summary for\nexample_index_to_summarize = 200\n\n# Create the prompt for few-shot inference\nfew_shot_prompt = make_prompt(full_examples_indices, example_index_to_summarize)\n\nprint(few_shot_prompt)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:52.117650Z","iopub.execute_input":"2023-10-14T22:33:52.117990Z","iopub.status.idle":"2023-10-14T22:33:52.124986Z","shell.execute_reply.started":"2023-10-14T22:33:52.117956Z","shell.execute_reply":"2023-10-14T22:33:52.124167Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nDialogue:\n\n#Person1#: What time is it, Tom?\n#Person2#: Just a minute. It's ten to nine by my watch.\n#Person1#: Is it? I had no idea it was so late. I must be off now.\n#Person2#: What's the hurry?\n#Person1#: I must catch the nine-thirty train.\n#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n\nWhat was going on?\n#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n\n\n\nDialogue:\n\n#Person1#: May, do you mind helping me prepare for the picnic?\n#Person2#: Sure. Have you checked the weather report?\n#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n#Person1#: Okay. Please take some fruit salad and crackers for me.\n#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n#Person1#: All set. May, can you help me take all these things to the living room?\n#Person2#: Yes, madam.\n#Person1#: Ask Daniel to give you a hand?\n#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n\nWhat was going on?\nMom asks May to help to prepare for the picnic and May agrees.\n\n\n\nDialogue:\n\n#Person1#: Hello, I bought the pendant in your shop, just before. \n#Person2#: Yes. Thank you very much. \n#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n#Person2#: Oh, is it? \n#Person1#: Would you change it to a new one? \n#Person2#: Yes, certainly. You have the receipt? \n#Person1#: Yes, I do. \n#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n#Person1#: Thank you so much. \n\nWhat was going on?\n#Person1# wants to change the broken pendant in #Person2#'s shop.\n\n\n\nDialogue:\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nWhat was going on?\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now pass this prompt to perform a few shot inference:","metadata":{"tags":[]}},{"cell_type":"code","source":"# Retrieve the human-generated summary for the specified example\nsummary = dataset['test'][example_index_to_summarize]['summary']\n\n# Tokenize the few-shot prompt and convert it to PyTorch tensors\ninputs = tokenizer(few_shot_prompt, return_tensors='pt')\n\n# Generate an output using the model, limiting the new tokens to 50\n# This uses the LLM to generate a summary of the dialogue with the few-shot prompt\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\n# Show the results\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:52.126495Z","iopub.execute_input":"2023-10-14T22:33:52.127354Z","iopub.status.idle":"2023-10-14T22:33:57.443115Z","shell.execute_reply.started":"2023-10-14T22:33:52.127312Z","shell.execute_reply":"2023-10-14T22:33:57.442156Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - FEW SHOT:\n#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this scenario, using few-shot inference didn't yield a significant improvement over one-shot inference. Moreover, going beyond 5 or 6 shots generally doesn't offer much help either. It's crucial to be mindful of not exceeding the model's input-context length, which, in our case, is 512 tokens. Any content beyond this context length will be disregarded.\n\nHowever, it's noticeable that including at least one full example (one shot) furnishes the model with additional information, resulting in a qualitative enhancement in the overall summary.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Feel free to experiment with few-shot inference:\n\nSelect different dialogues by modifying the indices in the example_indices_full list and the example_index_to_summarize value.\nAdjust the number of shots, ensuring it remains within the model's 512 context lengths for fair comparison.\n\nObserve how well few-shot inference performs with other examples.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='5'></a>\n# 5 - Configuration Parameters for Generative Inference\nIn this section, we'll delve into the various configuration parameters that play a crucial role in generative inference.\n\n\n\n\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Feel free to alter the configuration parameters of the **`generate()`** method to observe varied outputs from the LLM. Up until now, the only parameter set was **`max_new_tokens=50`**, determining the maximum number of tokens to generate. For a comprehensive list of available parameters, refer to the **[Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig)**.\n\nA convenient method for organizing configuration parameters is by using the **`GenerationConfig`** class.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Define a GenerationConfig with specific parameters\ngeneration_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n\n# Tokenize the few-shot prompt and convert it to PyTorch tensors\ninputs = tokenizer(few_shot_prompt, return_tensors='pt')\n\n# Generate an output using the model, limiting the new tokens to 50\n# This uses the LLM to generate a summary of the dialogue with the few-shot prompt\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config=generation_config,\n    )[0], \n    skip_special_tokens=True\n)\n\n# Show the results\nprint(dash_line)\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-10-14T22:33:57.444646Z","iopub.execute_input":"2023-10-14T22:33:57.444997Z","iopub.status.idle":"2023-10-14T22:34:00.706926Z","shell.execute_reply.started":"2023-10-14T22:33:57.444963Z","shell.execute_reply":"2023-10-14T22:34:00.705974Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nMODEL GENERATION - FEW SHOT:\n#Person1 wants to upgrade his system and hardware.\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Additionally, modify the configuration parameters to explore their impact on the output. By setting the parameter do_sample = True, you enable different decoding strategies that affect the selection of the next token from the probability distribution across the entire vocabulary. You can further adjust the outputs by tweaking parameters like temperature, as well as others such as top_k and top_p.","metadata":{}},{"cell_type":"markdown","source":"Comments regarding the parameter choices in the above code cell:\n\n- Opting for **`max_new_tokens=10`** will result in overly concise output text, potentially truncating the dialogue summary.\n- Enabling **`do_sample = True`** and adjusting the temperature value provides increased flexibility in the output.","metadata":{}},{"cell_type":"markdown","source":"As you can see, prompt engineering can take you a long way for this use case, but there are some limitations. While one-shot and few-shot inference strategies offer valuable insights, exceeding a certain number of shots may not necessarily improve performance, and it's crucial to stay within the model's context length.\n\nThe choice of parameters, such as do_sample, temperature, and max_new_tokens, significantly influences the generated output. Fine-tuning these parameters allows you to tailor the summary to your specific requirements, providing flexibility in adapting the model's behavior.\n\nIn the second notebook, we will explore how fine-tuning can be employed to enhance your LLM's understanding of a particular use case, unlocking even more potential for nuanced and accurate generative text. Stay tuned for a deeper dive into fine-tuning and its impact on model performance!","metadata":{}}]}