{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7701220,"sourceType":"datasetVersion","datasetId":9},{"sourceId":11359,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8749}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Starter Notebook: Generating More Data With Gemma\nOur ultimate goal in this competition is to take an original sample of text and a new version of that text rewritten by Gemma, and to figure out what prompt was used to get the new version. A helpful first step is to be able to generate a bunch of examples of what that looks like, so we can then learn the relationships between the original text, rewrite prompt and rewritten text.\n\nTo generate examples, we'll need a few things:\n1. A corpus of original texts\n2. A set of rewrite prompts\n3. Our model (Gemma!) to use the original text and rewrite prompt to generate a rewritten text\n\nLet's tackle them one by one.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Generating `original_text`\nWhile we don't know too much about the original text used in the competition test set,\nthe meta-kaggle dataset provides a corpus of forum messages on kaggle that we can\nuse as a simple example.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nforum_messsages_df = pd.read_csv('/kaggle/input/meta-kaggle/ForumMessages.csv')\nforum_messsages_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-13T00:27:51.662234Z","iopub.execute_input":"2024-03-13T00:27:51.662629Z","iopub.status.idle":"2024-03-13T00:28:17.489692Z","shell.execute_reply.started":"2024-03-13T00:27:51.662598Z","shell.execute_reply":"2024-03-13T00:28:17.488658Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"       Id  ForumTopicId  PostUserId             PostDate  \\\n0  667077        115913     1788308  11/06/2019 19:38:55   \n1  667076         74968     3961461  11/06/2019 19:38:19   \n2  667075        115817     1666986  11/06/2019 19:37:59   \n3  667074        113468     1073620  11/06/2019 19:34:36   \n4  667073        116025     1666986  11/06/2019 19:33:54   \n\n   ReplyToForumMessageId                                            Message  \\\n0               666668.0  <p><a href=\"/cdeotte\">@cdeotte</a> </p>\\n\\n<p>...   \n1                    NaN  <p>A very detailed and helpful notebook, \\nTha...   \n2                    NaN  <p>You don't say. You might just got your wish...   \n3               666591.0  <p>Hi <a href=\"/mobassir\">@mobassir</a>  If I ...   \n4                    NaN  <p>This like betting your life savings on a ga...   \n\n   Medal MedalAwardDate  \n0    3.0     11/06/2019  \n1    NaN            NaN  \n2    NaN            NaN  \n3    3.0     11/07/2019  \n4    3.0     11/06/2019  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ForumTopicId</th>\n      <th>PostUserId</th>\n      <th>PostDate</th>\n      <th>ReplyToForumMessageId</th>\n      <th>Message</th>\n      <th>Medal</th>\n      <th>MedalAwardDate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>667077</td>\n      <td>115913</td>\n      <td>1788308</td>\n      <td>11/06/2019 19:38:55</td>\n      <td>666668.0</td>\n      <td>&lt;p&gt;&lt;a href=\"/cdeotte\"&gt;@cdeotte&lt;/a&gt; &lt;/p&gt;\\n\\n&lt;p&gt;...</td>\n      <td>3.0</td>\n      <td>11/06/2019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>667076</td>\n      <td>74968</td>\n      <td>3961461</td>\n      <td>11/06/2019 19:38:19</td>\n      <td>NaN</td>\n      <td>&lt;p&gt;A very detailed and helpful notebook, \\nTha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>667075</td>\n      <td>115817</td>\n      <td>1666986</td>\n      <td>11/06/2019 19:37:59</td>\n      <td>NaN</td>\n      <td>&lt;p&gt;You don't say. You might just got your wish...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>667074</td>\n      <td>113468</td>\n      <td>1073620</td>\n      <td>11/06/2019 19:34:36</td>\n      <td>666591.0</td>\n      <td>&lt;p&gt;Hi &lt;a href=\"/mobassir\"&gt;@mobassir&lt;/a&gt;  If I ...</td>\n      <td>3.0</td>\n      <td>11/07/2019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>667073</td>\n      <td>116025</td>\n      <td>1666986</td>\n      <td>11/06/2019 19:33:54</td>\n      <td>NaN</td>\n      <td>&lt;p&gt;This like betting your life savings on a ga...</td>\n      <td>3.0</td>\n      <td>11/06/2019</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Let's grab the first 5 messages to test our generation pipeline:\n\noriginal_texts = forum_messsages_df['Message'][:5]","metadata":{"execution":{"iopub.status.busy":"2024-03-13T00:28:17.491702Z","iopub.execute_input":"2024-03-13T00:28:17.492080Z","iopub.status.idle":"2024-03-13T00:28:17.497702Z","shell.execute_reply.started":"2024-03-13T00:28:17.492043Z","shell.execute_reply":"2024-03-13T00:28:17.496880Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Generating `rewrite_prompt`\nWhile there are lots of ways to come up with rewrite prompts, for simplicity here are a few random prompts we can use.","metadata":{}},{"cell_type":"code","source":"rewrite_prompts = [\n    'Explain this to me like I\\'m five.',\n    'Convert this into a sea shanty.',\n    'Make this rhyme.',\n]","metadata":{"execution":{"iopub.status.busy":"2024-03-13T00:28:17.502389Z","iopub.execute_input":"2024-03-13T00:28:17.502707Z","iopub.status.idle":"2024-03-13T00:28:17.513545Z","shell.execute_reply.started":"2024-03-13T00:28:17.502683Z","shell.execute_reply":"2024-03-13T00:28:17.512553Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Generating `rewritten_text` with Gemma\nNow for the fun part! We can use gemma to rewrite our original text samples\nusing the rewrite prompts we created.\nThe code in this cell is borrowed from [the model card](https://www.kaggle.com/models/google/gemma/frameworks/pyTorch/variations/7b-it-quant).\nThe important things to know:\n\nWe're using the 7B parameter instruction tuned quantized model, which means:\n\n- 7B Parameter: this is the larger of the two Gemma models (the other has 2 billion parameters).\n    In general we expect the larger model to perform better on complex tasks, but\n    it's more resource intensive. You can see exactly how Gemma 7B compares to to Gemma 2B [here](https://ai.google.dev/gemma).\n- Instruction Tuned: instruction tuning is an extra training step that results in a model that\n    can follow user instructions better. Our rewrite prompt is a kind of instruction, so this is what we want!\n- Quantized: quantization is a way of shrinking the size of a model by reducing the precision of each\n    parameter; so while our model still has 7 billion parameters, it's easier to run on limited\n    hardware.\n\nAt the end of this cell, we'll have a `model` we can call `generate` on with a specially formatted prompt.","metadata":{}},{"cell_type":"code","source":"!pip install -q -U immutabledict sentencepiece \n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/\n\nimport sys \nsys.path.append(\"/kaggle/working/gemma_pytorch/\") \nfrom gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport contextlib\nimport os\nimport torch\n\n# Load the model\nVARIANT = \"7b-it-quant\" \nMACHINE_TYPE = \"cuda\" \nweights_dir = '/kaggle/input/gemma/pytorch/7b-it-quant/2' \n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\n# Model Config.\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\nmodel_config.quant = \"quant\" in VARIANT\n\n# Model.\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n  model = GemmaForCausalLM(model_config)\n  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n  model.load_weights(ckpt_path)\n  model = model.to(device).eval()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T00:28:30.472160Z","iopub.execute_input":"2024-03-13T00:28:30.472974Z","iopub.status.idle":"2024-03-13T00:28:52.542960Z","shell.execute_reply.started":"2024-03-13T00:28:30.472932Z","shell.execute_reply":"2024-03-13T00:28:52.541405Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'gemma_pytorch'...\nremote: Enumerating objects: 102, done.\u001b[K\nremote: Counting objects: 100% (47/47), done.\u001b[K\nremote: Compressing objects: 100% (33/33), done.\u001b[K\nremote: Total 102 (delta 26), reused 23 (delta 14), pack-reused 55\u001b[K\nReceiving objects: 100% (102/102), 2.15 MiB | 22.92 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(MACHINE_TYPE)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_default_tensor_type(model_config\u001b[38;5;241m.\u001b[39mget_dtype()):\n\u001b[0;32m---> 35\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43mGemmaForCausalLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m   ckpt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weights_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVARIANT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m   model\u001b[38;5;241m.\u001b[39mload_weights(ckpt_path)\n","File \u001b[0;32m/kaggle/working/gemma/model.py:400\u001b[0m, in \u001b[0;36mGemmaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m head_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m    398\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;241m=\u001b[39m Embedding(vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mquant)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m GemmaModel(config)\n","File \u001b[0;32m/kaggle/working/gemma/tokenizer.py:24\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path: Optional[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Reload tokenizer.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(model_path), model_path\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model \u001b[38;5;241m=\u001b[39m SentencePieceProcessor(model_file\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# BOS / EOS token IDs.\u001b[39;00m\n","\u001b[0;31mAssertionError\u001b[0m: /kaggle/input/gemma/pytorch/7b-it-quant/2/tokenizer.model"],"ename":"AssertionError","evalue":"/kaggle/input/gemma/pytorch/7b-it-quant/2/tokenizer.model","output_type":"error"}]},{"cell_type":"code","source":"# Now we can loop through our input texts, randomly select a rewrite prompt, and see Gemma in action:\n\nimport random\nrandom.seed(0)\n# This is the prompt format the model expects\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n\nrewrite_data = []\n\nfor original_text in original_texts:\n    rewrite_prompt = random.choice(rewrite_prompts)\n    prompt = f'{rewrite_prompt}\\n{original_text}'\n    rewritten_text = model.generate(\n        USER_CHAT_TEMPLATE.format(prompt=prompt),\n        device=device,\n        output_len=100,\n    )\n    rewrite_data.append({\n        'original_text': original_text,\n        'rewrite_prompt': rewrite_prompt,\n        'rewritten_text': rewritten_text,\n    })\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-13T00:28:52.543737Z","iopub.status.idle":"2024-03-13T00:28:52.544118Z","shell.execute_reply.started":"2024-03-13T00:28:52.543946Z","shell.execute_reply":"2024-03-13T00:28:52.543961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's turn our generated data into a dataframe, and spot check the first rewrite to see if it makes sense.\nrewrite_data_df = pd.DataFrame(rewrite_data)\nrewrite_data_df[:1].values","metadata":{"execution":{"iopub.status.busy":"2024-03-13T00:28:52.545588Z","iopub.status.idle":"2024-03-13T00:28:52.545951Z","shell.execute_reply.started":"2024-03-13T00:28:52.545758Z","shell.execute_reply":"2024-03-13T00:28:52.545772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next Steps\n\nHuzzah! We have a dataset with original texts, rewrite prompts, and rewritten text. Here are a couple of suggestions of next steps you could take to generate a larger, more diverse dataset:\n1. Add more original text data sources; besides just using all of the forum messages (instead of just the first 5), Kaggle has tons of datasets that would make reasonable input text. Here are few random datasets you could use:\n    - The `Plot` column from the [Wikipedia Movie Plots dataset](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots).\n    - The `text` column from the [Emotions dataset](https://www.kaggle.com/datasets/nelgiriyewithana/emotions).\n    - The `body_text` and `abstract` columns of the [Wikibooks Dataset](https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset).\n    \n    Note that each of these may need different preprocessing; for example, Gemma has a context length of 8192 tokens, so if the text is long, you'll need to truncate it.\n2. Use gemma to generate original text.\n3. Expand the list of rewrite prompts. You can come up with them manually, or explore having Gemma write rewrite prompts.\n4. Play around with the generation of `rewritten_text`:\n   - How does changing `output_len` affect the length and quality of rewrites?\n   - Do rewrites with the 2B parameter model differ substantially from the 7B model?\n   - Can you use [few shot prompting](https://www.promptingguide.ai/techniques/fewshot) to get higher quality rewrites?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}