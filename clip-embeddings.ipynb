{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Model \n- Training\n    backbone(CLIP) + Dropout + Dense(units = 256) + Arcface + Softmax (classes = 17691)\n- Inference\n    backbone(CLIP) + Dropout + Dense(units = 256) + AdaptiveAveragePooling(n=64)","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n\nimport re\nimport os \nimport glob\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport random \nimport math \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers \nfrom sklearn import metrics \nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nfrom tqdm.auto import tqdm\nfrom sklearn.preprocessing import normalize\n\nimport pickle\nimport json\nimport tensorflow_hub as tfhub\nfrom datetime import datetime\nimport gc\nimport requests\nfrom mpl_toolkits import axes_grid1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Device","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Running on TPU\", tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse: \n    # Default TF works on CPU and GPU\n    strategy = tf.distribute.get_strategy()\n    \nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"Replicas: \", strategy.num_replicas_in_sync)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If GPU instance it makes mixed precision enable \nif strategy.num_replicas_in_sync == 1:\n    from tensorflow.keras.mixed_precision import experimnetal as mix_precision\n    policy = mixed_precition.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    VERSION = 3\n    SUBV = \"Clip_ViT_Train\"\n    \n    SEED = 42\n    \n    # pretrained Model\n    RESUME = True\n    RESUME_EPOCH = 0\n    RESUME_WEIGHT = \"../input/guei-v6-clip-vit-large-arcface-train-projection/clip-vit-large-patch14_224pix-emb256_arcface_entire.h5\"\n    \n    #backbone model \n    model_type = \"clip-vit-large-patch14\"\n    EFF_SIZE = 0\n    EFF2_TYPE = \"\"\n    IMAGE_SIZE = 224\n    \n    # projection layer \n    N_CLASSES = 17691\n    EMB_DIM = 256\n    \n    # training \n    TRAIN = False\n    BATCH_SIZE = 200 * strategy.num_replicas_in_sync\n    EPOCHS = 100\n    LR = 0.001\n    save_dir = \"./\"\n    \n    DEBUG = False\n    \n# Function to seed everything \ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# model name \nMODEL_NAME = None\nif config.model_type == \"effnetv1\":\n    MODEL_NAME = f\"effnet_v1_b{config.EFF_SIZE}\"\nelif config.model_type == \"effnetv2\":\n    MODEL_NAME = f\"effnetv2_{config.EFF2_TYPE}\"\nelif \"swin\" in config.model_type:\n    MODEL_NAME = config.model_type\nelif \"conv\" in config.model_tpye:\n    MODEL_NAME = config.model_type\nelse:\n    MODEL_NAME = config.model_type\n    \nconfig.MODEL_NAME = MODEL_NAME\nprint(MODEL_NAME)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TFRecords","metadata":{}},{"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_shard_suffix = '*-train-*.tfrec'\n\nROOT_DIRS = [\n    \"guie-glr2021mini-tfrecords-label-10691-17690\",\n    \"guie-imagenet1k-mini1-tfrecords-label-0-999\",\n    \"guie-products10k-tfrecords-label-1000-10690\",\n]\n\ntrain_set_path = []\nvalid_set_path = []\nfor ROOT_DIR in ROOT_DIRS:\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path(ROOT_DIR)\n    \n    print(f\"\\\"{ROOT_DIR}\\\" : \\\"{GCS_DS_PATH}\\\", \")\n    files = sorted(tf.io.gfile.glob(GCS_DS_PATH + f\"/{train_shared_suffix}\"))\n    # split data \n    train_set_path += random.samples(files, int(len(files) * 0.9 ))\n    valid_set_path += [ file for file in files if not file in train_set_path ]\n    print(ROOT_DIR, \", number of tfrecords\", len(files))\n\ntrain_set_path = sorted( train_set_path )\nvalid_set_path = sorted( valid_set_path )\nprint(\"# of tfrecords for training: \", len(train_set_path))\nprint(\"# of tfrecords for training: \", len(valid_set_path))\n\nif config.DEBUG:\n    train_set_path = random.sample( train_set_path, 4)\n    print(\"Debug: reduce training data. num = \", len(trian_set_path))\n    \n    valid_set_path = train_set_path\n    print(\"debug: reduce validation data. num = \", len(valid_set_path))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_num_of_image(file):\n    return int(file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])\n\ntrain_set_len = sum( [ get_num_of_images(file) for file in train_set_path ] )\nvalid_set_len = sum( [ get_num_of_images(file) for file in valid_set_path ] )\n\ntrain_set_len, valid_set_len","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Pipeline ","metadata":{}},{"cell_type":"code","source":"def deserialization_fn(serialized_example):\n    parsed_example = tf.io.parse_single_example(serialized_example,\n                                               features = {\n                                                   'image/encoded': tf.io.FixedLenFeature([],tf.string),\n                                                   'image/class/label' : tf.io.FixedLenFeature([], tf.int64),\n                                               })\n    image = tf.image.decode_jpeg(parsed_example['image/encoded'], channels = 3)\n    image = tf.image.resize(image, size=(config.IMAGE_SIZE, config.IMAGE_SIZE))\n    label = tf.cast(parsed_example['image/class/label'], tf.int64)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def arcface_format(image, label_group):\n    return {'inp1': image, 'inp2' : label_group}, label_group\n\ndef rescale_image(image, label_group):\n    image = tf.cast(image, tf.float32) * 255.0\n    return image, label_group\n\n# Data augmentation \ndef data_augment(image, label_group):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_hue(image, 0.01)\n    image = tf.image.random_saturation(image, 0.70, 1.30)\n    image = tf.image.random_contrast(image, 0.80, 1.20)\n    image = tf.image.random_brightness(image, 0.10)\n    return image, label_group\n\n# Dataset to obtain backbone's inference \ndef get_backbone_inference_dataset(tfrecord_paths, cache = False,\n                                  repeat = False, shuffle = False, augment = False):\n    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths)\n    data_len = sum( [ get_num_of_images(file) for file in tfrecord_paths ] )\n    dataset = dataset.shuffle( data_len//10 ) if shuffle else dataset\n    dataset = dataset.flat_map(tf.data.TFRecordDataset)\n    dataset = dataset.map(deserialization_fn, num_parallel_calls = AUTO)\n    \n    if augment:\n        dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n        dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)\n        dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n        \n        if repeat:\n            dataset = dataset.repeat()\n        dataset = dataset.batch(config.BATCH_SIZE)\n        dataset = dataset.prefetch(AUTO)\n        return dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Viz tfrecord images","metadata":{}},{"cell_type":"code","source":"backbone_infer_dataset_encode = get_backbone_inference_dataset(train_set_path, shuffle = True, augment = True)\n\nnum_cols = 3\nnum_rows = 5\nbackbone_infer_dataset_encode = backbone_infer_dataset_encode.unbatch().batch(num_cols * num_rows)\nX,y = next(iter(backbone_infer_dataset_encode))\nprint(x['inp1'].shape)\n\nfig = plt.figure(figsize=(15,15))\ngrid = axes_grid1.ImageGrid(fig, 111, nrows_ncols = (num_cols, num_rows), axes_pad=0.1)\n\nfor i, ax in enumerate(grid):\n    ax.imshow(x['inp1'][i]/255)\n    ax.axis(\"off\")\n    \ndel backbone_infer_dataset_encode\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    \"\"\"\n    Implements large margin arc distance.\n    \n    Referece:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    \"\"\"\n    \n    def __init__(self, n_classes, s = 30, m=0.50, easy_margin = False,\n                ls_eps = 0.0, **kwargs):\n        super(ArcMarginProduct, self).__init__(**kwargs)\n        \n        self.n_classes = n_classes\n        self.s = s\n        self.m = m \n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n        \n    def get_config(self):\n        \n        config = super().get_config().copy()\n        config.update({\n            'n_classes' : self.n_classes,\n            's' : self.s,\n            'm' : self.m,\n            'ls_eps' : self.ls_eps,\n            'easy_margin' : self.easy_margin\n        })\n        return config\n    \n    def build(self, input_shape):\n        super(ArcMArginProduct, self).build(input_shape[0])\n        \n        self.W = self.add_weight(\n            name = 'W',\n            shape = (int(input_shape[0][-1]), self.n_classes),\n            initializer = 'glorot_uniform',\n            dtype = 'float32',\n            trainable = True,\n            regularizer = None\n        )\n        \n    def call(self, inputs):\n        X,y = inputs \n        y = tf.cast(y, dtype = tf.int32)\n        cosine = tf.matmul(\n            tf.math.12_normalize(X, axis = 1),\n            tf.math.12_normalize(self.W, axis = 0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.activity_regularizermm)\n        \n        one_hot = tf.cast(\n            tf.one_hot(y, depth = self.n_classes),\n            dtype = cosine.dtype\n        )\n        \n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n            \n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-03T22:25:19.359112Z","iopub.execute_input":"2023-11-03T22:25:19.359532Z","iopub.status.idle":"2023-11-03T22:25:19.826071Z","shell.execute_reply.started":"2023-11-03T22:25:19.359499Z","shell.execute_reply":"2023-11-03T22:25:19.824525Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_scale_layer(rescale = \"tf\"):\n    \n    if isinstance(rescale_mode, (list, tuple)):\n        mean, std = rescale_mode\n    elif rescale_mode == \"torch\":\n        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n        std = np.array([0.229, 0.224, 0.225]) * 255.0\n    elif rescale_mode == \"tf\":\n        mean, std = 127.5, 127.5\n    elif rescale_mode == \"tf128\":\n        mean, std = 128.0, 128.0\n    elif rescale_mode == \"raw01\":\n        mean, std = 0, 255.0\n    else:\n        mean, std = 0, 1\n    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std)\n    \n    return scaling_layer\n\ndef get_clip_model():\n    inp = tf.keras.layers.Input(shape = [3, 224, 224])\n    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n    output = backbone({'pixel_value' : inp}).pooler_output\n    return tf.keras.Model(inputs=[inp], outputs = [output])\n\ndef get_embedding_model():\n    \n    inp = tf.keras.layers.Input(shape = [None, None, 3], name = \"inp1\")\n    label = tf.keras.layers.Input(shape = {}, name = 'inp2')\n    \n    # Definition of layers\n    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name = 'resize')\n    layer_scaling = get_scale_layer(rescale_mode = 'torch')\n    layer_permute = tf.keras.layers.Permute((3,1,2))\n    layer_backbone = get_clip_model()\n    layer_dropout = tf.keras.layers.Dropout(0.2)\n    layer_dense_before_arcface = tf.keras.layers.Dense(config.EMB_DIM)\n    layer_margin = ArcMarginProduct(\n        n_classes = config.N_CLASSES,\n        s = 30,\n        m = 0.3,\n        name = f\"head/arcface\",\n        dtype = 'float32'\n    )\n    layer_softmax = tf.keras.layers.Softmax(dtype = 'float32')\n    layer_12 = tf.keras.layers.Lambda(lambda x: tf.math.12_normalize(x, axis = 1, name = 'embdedding_norm'))\n    \n    if config.EMB_DIM != 64:\n        layers_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n    else:\n        layer_adaptive_pooling = tf.keras.layers.Lambda(lambda x: x)\n        \n    image = layer_scaling(inp)\n    image = layer_resize(image)\n    image = layer_permute(image)\n    backbone_output = layer_backbone(image)\n    embed = layer_dropout(backbone_output)\n    embed = layer_dense_before_arcface(embed)\n    x = layer_margin([embed, label])\n    output = layer_softmax(x)\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    \n    model.layers[-6].trainable = False\n    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n    model.compile(\n        optimizer = opt,\n        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n        metrics = [tf.keras.metrics.SpaceCategoricalAccuracy(), tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n    )\n    \n    # Definition of Embedding model \n    embed_model = keras.Sequential([\n        keras.layers.InputLayer(input_shape = (None, None, 3), dtype = 'uint8'),\n        layer_scaling,\n        layer_resize,\n        layer_permute,\n        layer_backbone,\n        layer_dropout,\n        layer_dense_before_arcface,\n        layer_adaptive_pooling,\n        layer_12,\n        \n    ])\n    \n    return model, embed_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model, emb_model = get_embedding_model()\n    \nif config.RESUME:\n    print(f\"load {config.RESUME_WEIGHT}\")\n    model.load_weights(config.RESUME_WEIGHT)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scheduler","metadata":{}},{"cell_type":"code","source":"def get_lr_callback(plot = False):\n    lr_start = 0.000001\n    lr_max = 0.000005 * config.BATCH_SIZE\n    lr_min = 0.000001\n    lr_ramp_ep = 4\n    lr_sus_ep = 0\n    lr_decay = 0.95\n    \n    def lrfn(epoch):\n        if config.RESUME:\n            epoch = epoch + config.RESUME_EPOCH\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n    \n    if plot:\n        epochs = list(range(config.EPOCHS))\n        learning_rates = [lrfn(x) for x in epochs]\n        plt.scatter(epochs, learning_rates)\n        plt.show()\n        \n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n    return lr_callback\n\nget_lr_callback(plot = True)        ","metadata":{},"execution_count":null,"outputs":[]}]}