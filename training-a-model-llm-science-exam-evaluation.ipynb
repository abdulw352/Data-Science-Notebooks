{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training an Open Book Model for Q & A","metadata":{}},{"cell_type":"markdown","source":"For training a model for use with Open Book Q&A need a csv that contains: `prompt` (\"questions\"), `A,B,C,D,E` (answer choices), and also a column with the `context`.","metadata":{}},{"cell_type":"code","source":"import os \nos.environ['CUDA_VISIBLE_DEVICES'] = \"0.1\"\n\nfrom typing import Optional, Union\nimport pandas as pd, numpy as np, torch\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers import AutoTokenizer, EarlyStoppingCallback, AutoModelForMultipleChoice, TrainingmArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n\nVER = 2\n# Train with subset of 60K\nNUM_TRAIN_SAMPLES = 1_024\n# Parameter efficient fine tuning \n\nUSE_PEFT = False\n\nFREEZE_LAYERS = 18\n\nFREEZE_EMBEDDINGS = True\n\nMAX_INPUT = 256\n\nMODEL = 'microsoft/deberta-v3-large'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid = pd.read_csv(\"/kaggle/input/60k-data-with-context-v2/train_with_context2.csv\")\nprint(\"df shape\", df_valid.shape)\n\ndf_valid.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/60k-data-with-context-v2/all_12_with_context2.csv\")\n#df_train = df_train.drop(columns=\"source\")\n#df_train = df_train.fllna(\"\").sample(NUM_TRAIN_SAMPLES)\nprint(\"Train data size: \", df_train.shape)\ndf_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loader","metadata":{}},{"cell_type":"code","source":"option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\nindex_to_option = {v: k for k,v in option_to_index.item()}\n\ndef preprocess(example):\n    first_sentence = ['[CLS]' + example['context'] ] * 5\n    second_sentences = [\" ####\" + example['prompt'] + \" [SEP]\" + example[option] + \" [SEP]\" for option in \"ABCDE\"]\n    tokenized_example = tokenizer(first_sentence, second_sentences, truncation = \"only_first\", max_length=MAX_INPUT,\n                                 add_special_tokens = False)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    \n    return tokenized_example\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k,v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        batch = {k:v.view(batch_size, num_choices, -1) for k,v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL)\ndataset_valid = Dataset.from_pandas(df_valid)\ndataset = Dataset.from_pandas(df_train)\ndataset = dataset.remove_columns(['__index_level_0__'])\ndataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt','context','A','B','C','D','E', 'answer'])\ntokenized_dataset = dataset.map(preprocess, remove_columns=['prompt','context','A','B','C','D','E','answer'])\ntokenized_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model\n\nusing Hugging Face AutoModelForMultipleChoice. \noptionally can also use PEFT to accelerate training and uses less memory. However it has noticed that validation accuracy is lower. \nCan also feeze layers to accelerate training and lower memory use but this can also result in worse validation accuracy. ","metadata":{}},{"cell_type":"code","source":"model = AutoModelForMultipleChoice.from_pretrained(MODEL)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FREEZE_EMBEDDINGS:\n    print('Freezing Embeddings --------------------------------')\n    for param in model.deberta.embeddings.parameters():\n        param.requires_grad = False\nif FREEZE_LAYERS > 0:\n    print(f\"Freezing {FREEZE_LAYERS} layers. ---------------------------------------\")\n    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n        for param in layer.parameters():\n            param.requires_grad = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MAP@3 Metric ","metadata":{}},{"cell_type":"code","source":"def map_at_3(predictions, labels):\n    map_sum = 0\n    pred = np.argsort(-1*np.array(predictions, axis = 1)[:,:3])\n    for x,y in zip(pred,labels):\n        z = [1/i if y == j else 0 for i,j in zip([1,2,3],x)]\n        map_sum += np.sum(z)\n    return map_sum / len(predictions)\n\ndef compute_metrics(p):\n    predictions = p.predictions.tolist()\n    labels = p.label_ids.tolist()\n    return {\"map@3\" : map_at_3(predictions, labels)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and Save\n\nTricks to train model more efficiently when low RAM\n\n- use fp16 (speeds up T4 not p100)\n- use `gradient_accumulation_steps` (this simulates large batch sizes)\n- use `gradient_checkpointing` (this uses disk to save RAM)\n- freeze model embeddings (this reduces weights to train)\n- freeze some model layers (this reduces weights to train)\n- use PEFT (reduce weights to train)\n- increase LR and decrease epochs (this reduces work)\n- use smaller model (this reduces weights to train)","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    warmup_ratio = 0.1, \n    learning_rate = 2e-5,\n    per_device_train_batch_size = 1,\n    per_device_eval_batch_size = 2,\n    num_train_epochs = 2,\n    report_to = 'none',\n    output_dir = f'./checkpoints_{VER}',\n    overwrite_output_dir = True,\n    fp16 = True,\n    gradient_accumulation_steps = 8,\n    logging_steps = 25,\n    evaluation_strategy = 'steps',\n    eval_steps = 25,\n    save_strategy = 'steps',\n    save_steps = 25,\n    load_best_model_at_end = False,\n    metric_for_best_model = 'map@3',\n    lr_scheduler_type = 'cosine',\n    weight_decay = 0.01,\n    save_total_limit = 2,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model = model,\n    args = training_args,\n    tokenizer = tokenizer,\n    data_collator = DataCollatorForMultipleChoice(tokenizer = tokenizer),\n    train_dataset = tokenized_dataset,\n    eval_dataset = tokenized_dataset_valid,\n    compute_metrics = compute_metrics,\n)\n\ntrainer.train()\ntrainer.save_model(f'model_v{VER}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Verifying saved Model\n\nChecking to see if the model saved correctly","metadata":{}},{"cell_type":"code","source":"load_model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\ntrainer = Trainer(model = load_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/60k-data-with-context-v2/train_with_context2.csv\")\ntokenized_test_dataset = Dataset.from_pandas(test_df).map(\npreprocess, remove_columns = ['prompt','context','A','B','C','D','E'])\n\ntest_predictions = trainer.predict(tokenized_test_dataset).predictions\npredictions_as_ids = np.argsort(-test_predictions, 1)\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\npredictions_as_string = test_df['prediction'] = [' '.join(row) for row in predictions_as_answer_letters[:,:3]]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compute Validation Score","metadata":{}},{"cell_type":"code","source":"def precirions_at_k(r, k):\n    \"\"\"Precision at k\"\"\"\n    assert k <= len(r)\n    assert k != 0\n    return sum(int(x) for x in r[:k]) / k\n\ndef MAP_at_3(predictions, true_items):\n    \"\"\"Score is mean average precision at 3\"\"\"\n    U = len(predictions)\n    map_at_3 = 0.0\n    for u in range(U):\n        user_preds = predictions[u].split()\n        user_true = true_items[u]\n        user_results = [1 if item == user_true else 0 for item in user_preds]\n        for k in range(min(len(user_preds), 3)):\n            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n    return map_at_3 / U","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\nprint(\"CV MAP@3 = \", m)","metadata":{},"execution_count":null,"outputs":[]}]}